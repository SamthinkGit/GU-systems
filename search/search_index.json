{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udf1f GU-Systems: An ECM approximation to AGI\ud83c\udf1f","text":""},{"location":"#introduction","title":"\ud83d\ude80 Introduction","text":"<p>Welcome to the main repository for our cutting-edge project on the implementation of ECM (Execution-Cognition Machine) as a strategic approach towards the development of Computer Autonomous AI. This initiative represents a significant leap in crafting an intelligent system that not only mimic but also evolve human-like cognitive functionalities autonomously in computer interaction.</p> <p>\ud83e\udde0 The project revolves around designing, testing, and refining theoretical models that translate complex human problem-solving capabilities into computational strategies executable by AI. We integrate advanced AI frameworks. Our system architecture is robust, accommodating rapid iterations and rigorous validations through continuous integration and deployment mechanisms.</p> <p>\ud83d\udcda For detailed insights into our methodologies and system functionalities, please refer to the README, the documentation included in this repository or surf through the pages of this Wiki.</p>"},{"location":"Action-Space/","title":"Introduction to the Action Space","text":"<p>The Action Space is the unified interface through which agents and developers interact with the system's capabilities. It contains both:</p> <ul> <li>Actions: Simple, typed, AI-facing functions intended to be used by agents.</li> <li>Tools: Developer-facing utilities that offer more flexibility and return complex types such as images or data structures.</li> </ul> <p>Both types are registered and accessed exclusively through the <code>ItemRegistry</code>. For more on how the ItemRegistry works, refer to the official ItemRegistry entry</p>"},{"location":"Action-Space/#why-use-the-itemregistry","title":"Why Use the ItemRegistry","text":"<ul> <li>Ensures compatibility across local and remote environments. When the ItemRegistry is wrapped (e.g., with <code>wrap_item_registry</code>), all registered functions are automatically redirected to their remote counterparts.</li> <li>Allows runtime introspection, replacement, and mocking of registered functions.</li> <li>Provides consistent loading, grouping, and aliasing of callable items.</li> <li>Enables modularization via packages, allowing grouped loading of related functions.</li> </ul> <p>Calling a function directly bypasses these benefits and may lead to executing outdated or incompatible code. Always access functions through the registry.</p>"},{"location":"Action-Space/#action-naming-convention","title":"Action Naming Convention","text":"<p>Actions and tools in the Action Space follow the naming convention:</p> <pre><code>&lt;dir&gt;/&lt;subdir&gt;.&lt;action_name&gt;()\n</code></pre> <p>This structure indicates:</p> <ul> <li>The folder where the function is defined.</li> <li>The action name.</li> </ul> <p>Note: The folder structure doesn't always match the package name. Each action file contains a <code>PKG_NAME</code> (e.g., <code>PKG_NAME = \"sleep\"</code>), which is the true package identifier.</p> <p>The following example:</p> <pre><code>vision/moondream.ask_to_image(\"VSCode\")\n</code></pre> <p>Is equivalent to the following code</p> <pre><code>&gt;&gt;&gt; import action_space.vision.moondream.actions\n&gt;&gt;&gt; import action_space.experimental.screenshot.actions  # Dependency\n&gt;&gt;&gt; from ecm.tools.registry import ItemRegistry\n&gt;&gt;&gt;\n&gt;&gt;&gt; registry = ItemRegistry()\n&gt;&gt;&gt; registry.load_all()\n\n&gt;&gt;&gt; action = registry.get(\"ask_to_image\", type=\"action\")\n&gt;&gt;&gt; ask_to_image = action.content\n&gt;&gt;&gt; result = ask_to_image(\"Is VSCode Open?\")\n&gt;&gt;&gt; print(result)\nyes\n</code></pre>"},{"location":"Action-Space/#key-principles","title":"Key Principles","text":"<ul> <li> <p>Actions:</p> </li> <li> <p>Intended for agents</p> </li> <li>Always return primitive types (<code>str</code>, <code>int</code>, <code>bool</code>)</li> <li> <p>Must be simple, typed, and easy to describe</p> </li> <li> <p>Tools:</p> </li> <li> <p>Intended for developers and environments</p> </li> <li>Can return complex objects but they must be serializable</li> <li>Often used to prepare or analyze environment state (e.g., <code>screenshot()</code>)</li> </ul> <p>All tools and actions\u2014whether local or executed on a remote system\u2014must be accessed through the <code>ItemRegistry</code> to ensure they execute on the machine being controlled by the agent.</p>"},{"location":"Action-Space/#summary","title":"Summary","text":"<ul> <li>The Action Space contains tools and actions registered via <code>ItemRegistry</code>.</li> <li>Always use <code>&lt;dir&gt;/&lt;subdir&gt;.&lt;name&gt;()</code> format to call registered items.</li> <li>Only functions inside the registry benefit from remote redirection and runtime enhancement.</li> <li>Tools = developer use. Actions = agent use.</li> <li>Actions only return simple types. Tools can return anything. However all types must be serializable (str, bool, int, etc)</li> </ul> <p>This standardization enables composability, portability, and control across diverse agent execution contexts.</p>"},{"location":"AgentProtocol/","title":"AgentProtocol","text":"<p>The Agent Protocol is an integral part of the ECM architecture which has also defined key modules for agents that have shown the best outcomes in various sectors. In this way the main goal of AP is solving the lack of a standardized set of rules or guidelines in Agent building. A problem that would make agents impractical to reimplement for new tasks or in different sectors.</p>"},{"location":"AgentProtocol/#purpose-of-the-agent-protocol","title":"Purpose of the Agent Protocol","text":"<p>The Agent Protocol is an open-source project that establishes a set of minimally intrusive rules on the architectures of Artificial General Intelligences (AGIs) to define the expected behavior externally, while maintaining a \"black box\" approach to their internal implementation. This standardization facilitates the reuse and adaptation of agent designs across different platforms and tasks without needing to understand or alter the underlying codebase.</p> <p>To conform to the Agent Protocol, an AGI must implement several key features:</p> <ul> <li>REST API: The agent should feature a RESTful API that external users can interact with. This API eliminates dependencies on the code and language, making the original design reusable and accessible.</li> </ul> <p>The REST API includes specific endpoints for task management and step execution, which standardize interactions and simplify the integration process:</p> <ul> <li> <p><code>/ap/v1/agent/tasks [POST]</code>: This endpoint is used to create new specific tasks that the agent must execute. It accepts a detailed description of the task, including necessary inputs for its execution.</p> </li> <li> <p><code>/ap/v1/agent/tasks/{task_id}/steps [POST]</code>: Designed to trigger the next step in a task's execution. This allows for sequential and detailed control over the processes carried out by the agent, ensuring effective tracking of progress and task completion.</p> </li> </ul>"},{"location":"AgentProtocol/#implementation-details-of-the-agent-protocol","title":"Implementation Details of the Agent Protocol","text":"<p>The implementation of the Agent Protocol includes predefined response mechanisms. These ensure that any agent can consistently report the status of tasks and steps, results obtained, or any artifacts generated during the process. These mechanisms are designed to be simple and flexible, adaptable to different technologies and frameworks without compromising functionality.</p> <p>To encourage broad adoption and ease the integration of agents from different platforms, the protocol also provides a set of SDKs. These SDKs abstract the complexities of direct API implementation, offering a simplified interface that allows developers to focus on the high-level logic of their agents without getting bogged down in communication details or API syntax.</p>"},{"location":"AgentProtocol/#autogpt-and-the-agent-protocol","title":"AutoGPT and the Agent Protocol","text":"<p>Thanks to this protocol, AutoGPT has launched a framework called \"Forge,\" which greatly facilitates the creation of agents aligned with the Agent Protocol. Forge acts as a scaffolding that developers can use to build their agents, ensuring they meet the standards set forth by the Agent Protocol while focusing on innovation and functionality.</p>"},{"location":"AgentProtocol/#conclusion","title":"Conclusion","text":"<p>The Agent Protocol serves as a foundational component in the ecosystem of ECM/AutoGPT, providing a standardized approach to agent development. This standardization is crucial for scaling the implementation of intelligent agents across different domains and platforms, ensuring interoperability and reusability.</p>"},{"location":"Architecture-Guide/","title":"Introduction","text":"<p>In this guide, you will gain a deep understanding of the ECM architecture implemented in this repository. We will explore the main modules and core approaches to solving the ECM problem. It is recommended that you first read the Theoretical Fundamentals and the Architecture Overview guides.</p> <p>The primary challenge of an ECM lies in synchronizing multiple layers. Although an ECM can be approached with various architectures, this repository divides all modules into four layers: - <code>/cognition_layer</code>: This directory contains all the AI agents responsible for thinking, planning, reasoning, and all necessary components to fully build and deploy those agents, generally adhering to the Agent Protocol Standard. - <code>/ecm</code>: This directory holds all the mediators, communicators, and middleware between the execution layer and the cognition layer. Only templates and virtual functions reside here, allowing the cognition layer to interact with the execution layer through any implementation using templates such as an Interpreter. Additionally, tools that can be used in both execution and cognition layers, such as the Exelent Parser or the Item Registry, are found here. - <code>/execution_layer</code>: This directory includes all modules that assist agents in executing commands, typically communicated using the Exelent language. Here, you can find thread managers, registries, callbacks, etc. - <code>/action_space</code>: This directory defines all the actions that agents can take. By importing these modules, you add new interactions to your agent, with actions resolved by the execution layer. Modules for keyboard interaction, window management, etc., can be found here.</p>"},{"location":"Architecture-Guide/#base-execution-diagram-for-the-ecm","title":"Base Execution Diagram for the ECM","text":"<p>Each layer in this architecture plays a crucial role in ensuring the efficiency, coherence, and functionality of the AI system. The following diagram illustrates the main execution flow of the ECM. The upper layers focus on cognitive interactions and decision-making, while the lower layers handle practical execution and management of specific functions. This separation of responsibilities allows for better organization and facilitates maintenance and scalability of the system.</p>"},{"location":"Architecture-Guide/#cognition-layer-agent","title":"Cognition Layer Agent","text":"<p>The Cognition Layer Agent is responsible for interacting with the LL. Its primary function is to handle problems by receiving a user query and generating a response to that problem, generally returning Exelent code. For this implementation, the agent can use tools, multiple steps, requests to the LLM, etc. Regardless of the agent's internal workings, the external interface behaves as an iterator, where each step of the agent is controlled by lower layers.</p> <p>Some implementations of this layer can be found in the /cognition_layer/[module]/agents directory, where each agent follows different architectures. Here are some examples:</p> <ul> <li>/planex: Planex uses three different agents (Planification, Reduction, Translation) executed sequentially. Each agent contains a function <code>.plan()</code>, <code>.reduce()</code>, or <code>.translate()</code> to chain the generated data from the LLM, returning a string with the generated plan.</li> <li>/planexv2: PlanexV2 reuses the code of the original Planex agent but implements a new <code>Blamer</code> agent that checks if the code is correct. If not, it executes the failed agent again until obtaining a valid plan.</li> <li>/RePlan: RePlan uses ReAct architecture to control the execution of the generated code. It uses PlanexV2 for generating Exelent files, and once generated, it manages the interpreters to approve, control, and check the execution of the plan.</li> </ul> <p>Note that all these agents depend on a set of prompts usually found in the directory /cognition_layer/[agent]/agents/prompts.py.</p> <p>Some implementations of these agents in action can be found in the /tests/sandbox directory, where you can experiment with different queries.</p> <p>Note also how some of these agents implement an <code>iter()</code> function, which returns a generator allowing you to easily execute all the steps of the agent. The messages returned may vary between different agents, but they can implement a Response model as a dataclass, where you can obtain information about each step.</p> <pre><code>agent = RePlan()\nstep: ReplanResponse\n\n# In this case the generator is async, but this doesn't need to be true for all agents\nasync for step in replan.iter(query=\"Open the terminal in linux\"):\n    print(f\"[{step.name}]\" + Style.RESET_ALL)\n    print(step.content)\n</code></pre> <p>Further information about each agent can be found on the same file or by checking the wiki if it the agent has already been published.</p>"},{"location":"Architecture-Guide/#fastagentprotocol","title":"FastAgentProtocol","text":"<p>The <code>FastAgentProtocol</code> is a simplified and faster alternative to the previous <code>AgentProtocol</code>, which required server setups and remote API configurations. The new approach eliminates unnecessary complexity, enabling direct and efficient communication between agents and the main system.</p> <p>Note: This protocol can be found at /cognition_layer/protocols/fast_ap.py</p> <ul> <li>Simplification: No more server setup or port management.</li> <li>Performance: Local execution reduces latency.</li> <li>Standardization: Provides a unified, consistent framework for agent integration.</li> </ul> <p>The <code>FastAgentProtocol</code> acts as a mediator between the main system and cognitive agents. It leverages an iterator-based approach, where the agent defines an iterative process that yields step-by-step progress. Each step includes information such as the step's name, content, and whether it's the final step.</p> <p>More information can be found in the FastAgentProtocol section on the wiki</p>"},{"location":"Architecture-Guide/#ecm-core","title":"ECM Core","text":"<p>The Main Module or ECM Core can be found at /ecm/core and contains the main execution loop of the system. This component is the operational core where all modules are managed. Here, you can test all agents. These agents are implemented at the top of the file and will be executed until stopped by the user.</p> <p>Here are some properties of this module:</p> <ul> <li>This module does not depend on the implementations or internals of the cognition/execution layers; it uses only the virtual interfaces/APIs from the middlewares.</li> <li>By default, all agent actions are deactivated in the host for safety reasons. Instead of executing, the actions will only log a simulation of each action's execution (keyboard, mouse control, etc. CognitionLayer steps are not included, only Action space). If you want to run an agent on the host machine with full integration, use /ecm/core/run_in_host.py.</li> </ul> <p>You can run this module using the following command:</p> <pre><code>python ecm/core/main.py --debug --agent [Planex|PlanexV2|RePlan...]\n</code></pre> <p>You can also use <code>-h</code> for more information about the arguments.</p> <pre><code>python ecm/core/main.py -h\n</code></pre>"},{"location":"Architecture-Guide/#interpreter","title":"Interpreter","text":"<p>The Interpreter unifies all executors within a single framework, allowing the system to execute functions and read commands from Exelent files. All interpreters must satisfy the attributes defined in the virtual class found at /ecm/mediator/Interpreter.py. This means all interpreters must contain the following properties:</p> <ol> <li>Contain and inherit all the methods and attributes defined in the virtual Interpreter class.</li> <li>Define a supports class for assigning all properties that can be used (at least the contained properties, extendable as needed) by inheriting from the InterpreterSupports class.</li> <li>If the Interpreter supports feedback returning, it must return a Feedback Object that is a subclass, inheriting from the Feedback class defined in /ecm/mediator/feedback.py and defining all specified methods/properties. Note that all feedback generated must always contain an <code>_exec_code</code> to define the status of the execution, using the <code>ExecutionStatus(Enum)</code> class defined in /ecm/mediator/feedback.py.</li> </ol>"},{"location":"Architecture-Guide/#execution-layer-module","title":"Execution Layer Module","text":"<p>The Execution Layer Module manages the execution of functions within the system. This component coordinates and oversees the operation of various threads, functions, callbacks, etc., needed for building the interpreter and achieving the full execution of multiple tasks and different behaviors defined in the Exelent language. This module can vary significantly, making it challenging to establish a standard implementation. However, you can explore the ROSA Insights to further investigate the key problems of this layer.</p>"},{"location":"Architecture-Guide/#action-space","title":"Action Space","text":"<p>The Action Space consists of executable functions that can be run on the host. It represents the set of actions and commands available for the AI to carry out in response to requests and tasks assigned. It should contain multiple functions to facilitate the actions of the Agents. These functions have the following properties:</p> <ol> <li>They can raise feedback if the interpreter that calls those functions enables it, but it is better to directly return information to the agent and save statuses between calls.</li> <li>They must always receive <code>str</code> or <code>list[str]</code> as arguments and return <code>str</code> because they will be managed by an AI with words.</li> <li>They should be as simple and straightforward as possible. Avoid more than 2 arguments per function.</li> <li>They must always contain a docstring explaining their functionality and how to use them. This docstring will be shown to the agents.</li> </ol> <p>These actions can be defined anywhere, but it is standard to define them in the /action_space directory. To define them, you just have to use the Item Registry as follows:</p> <pre><code>from ecm.tools.registry import ItemRegistry\n\n@ItemRegistry.register(type=\"action\")\ndef hello_world():\n    \"\"\"Says hello world.\\nUsage: hello_world()\"\"\"  # noqa\n    print(\"Hello World!\")\n</code></pre> <p>If you want the Agents to have multiple names for using you function you can use the <code>alias</code> function of the Item Registry. This will improve the AI's ease and probability to use your tool.</p> <pre><code>@ItemRegistry.alias([\n  \"say_hello_world\",\n  \"greet\"\n])\n@ItemRegistry.register(type=\"action\")\ndef hello_world():\n    \"\"\"Says hello world.\\nUsage: hello_world()\"\"\"  # noqa\n    print(\"Hello World!\")\n</code></pre> <p>Note: Don't use more than 3 aliases if it isn't necessary, each alias results in more tokens parsed by the Agent.</p> <p>Finally, note that all these functions must be imported in order to be registered in the ItemRegistry. Therefore, in multiple files, you will find imports like the following, even though they are not used in the file:</p> <pre><code>import action_space.talk.greet # noqa\n</code></pre> <p>Note: More information about the ItemRegistry can be found on its own section at the wiki.</p>"},{"location":"Architecture-Guide/#conclusion","title":"Conclusion","text":"<p>This guide has provided a comprehensive overview of the ECM architecture implemented in this repository, detailing its layered structure and key components.</p> <p>Each layer and module plays a vital role in the overall architecture. By adhering to the defined standards and protocols, the system achieves a high level of integration and functionality, capable of synchronizing both cognition and execution layer.</p>"},{"location":"Bare%E2%80%90Metal-Installation/","title":"Installation of the ECM on a Minimal Bare-Metal System","text":"<p>This document provides a comprehensive and technically detailed methodology for deploying the ECM on a resource-constrained, headless embedded platform. As an example, the target hardware is the <code>NXPIMX8MPEVK</code> development board, which features a heterogeneous multi-core ARM Cortex-A53/A72 processor architecture within the i.MX8M Plus SoC family. This procedure is applicable to scenarios where graphical processing capabilities are minimal or absent and where deterministic, CPU-bound computational loads are predominant.</p> <p>The deployment leverages Arch Linux due to its modularity, lightweight nature, and suitability for environments requiring precise system customization. Nevertheless, the instructions can be extrapolated to any Linux distribution that incorporates a package management system, Wi-Fi driver support, and the capacity to compile and maintain a custom kernel.</p> <p>The objective is to enable ECM functionality on a system without relying on graphical interfaces or power-hungry frameworks, thereby optimizing performance for edge inference and computational workloads using only essential system services.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#1-partitioning-and-structuring-the-boot-medium","title":"1. Partitioning and Structuring the Boot Medium","text":"<p>Employ tools such as <code>fdisk</code>, <code>gdisk</code>, or <code>parted</code> to manually configure the target storage device\u2014either SD card or eMMC\u2014with a layout that facilitates both bootloader and operating system installation:</p> <ul> <li>Reserve the first 20480 bytes (20 KiB) at the beginning of the medium to accommodate the U-Boot bootloader. This sector must remain unallocated.</li> <li>Create a primary partition of type <code>ext4</code> beginning from byte offset 20480 and extending to roughly 1024000 bytes. This partition will host the kernel image (<code>Image</code>), device tree blobs (<code>.dtb</code>), and optionally, boot scripts (<code>boot.txt</code>, <code>boot.scr</code>).</li> <li>Allocate a second partition from approximately byte offset 1228800 onward. This will contain the root filesystem, inclusive of the ECM runtime and its dependencies.</li> </ul> <p>[!NOTE] This arrangement separates boot-critical components from the root filesystem, enabling cleaner updates and recovery.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#2-acquisition-of-the-bootloader-binary","title":"2. Acquisition of the Bootloader Binary","text":"<p>The bootloader for i.MX series processors, namely U-Boot, is distributed by NXP through its official software repositories. Navigate to:</p> <p>https://www.nxp.com/design/design-center/software/embedded-software/i-mx-software/embedded-linux-for-i-mx-applications-processors\\:IMXLINUX</p> <p>Select the appropriate version matching the hardware revision of your IMX8MPEVK board. Extract the binary image corresponding to the secondary program loader (SPL) and/or U-Boot proper.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#3-flashing-the-bootloader","title":"3. Flashing the Bootloader","text":"<p>Refer to the reference manual and NXP documentation to determine the boot offset. For i.MX8M EVK devices, the SoC expects to load the bootloader from a 32 KiB offset:</p> <pre><code>sudo dd if=&lt;U-Boot_image&gt; of=/dev/sdX bs=1k seek=32 conv=fsync\n</code></pre> <p>Ensure all write operations are flushed to disk. Improper flashing or incorrect offsets may result in non-bootable media.</p> <p>[!WARNING] Always verify <code>/dev/sdX</code> is the correct device path. Inadvertently writing to an incorrect block device may irreversibly corrupt existing data in your current machine.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#4-preparing-the-boot-partition-and-kernel-setup","title":"4. Preparing the Boot Partition and Kernel Setup","text":"<p>Mount the first partition and prepare it to receive the kernel binary and DTB artifacts. This step involves creating necessary directory structures (e.g., <code>/boot/</code>, <code>/dtbs/</code>) and verifying filesystem integrity via <code>fsck</code>. The layout must conform to the bootloader\u2019s expectations, particularly the device tree path and kernel entry point.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#5-kernel-compilation","title":"5. Kernel Compilation","text":"<p>Compile a custom kernel tailored to the exact hardware specifications and deployment requirements. Fetch the kernel source code maintained by NXP for i.MX platforms:</p> <p>https://github.com/nxp-imx/linux-imx</p> <p>Proceed with the following:</p> <ul> <li>Use <code>make menuconfig</code> to access the kernel configuration interface and fine-tune options such as Wi-Fi chipsets, file systems, and network drivers.</li> <li>Explicitly activate support for the Marvell 88W8997 chipset, typically found on IMX8M boards, under <code>CONFIG_MWIFIEX_SDIO</code> and related options.</li> <li>Cross-compile using a suitable toolchain, such as <code>aarch64-linux-gnu-gcc</code>, ensuring environmental variables (<code>ARCH</code>, <code>CROSS_COMPILE</code>) are correctly defined.</li> </ul> <p>[!NOTE] Consider saving the <code>.config</code> file and kernel headers for future kernel module compilation or system updates.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#6-generation-of-kernel-artifacts","title":"6. Generation of Kernel Artifacts","text":"<p>Post-compilation, extract and validate the following artifacts:</p> <ul> <li><code>Image</code>: the kernel binary to be loaded by U-Boot.</li> <li><code>.dtb</code>: Device Tree Blobs corresponding to the board configuration (e.g., <code>imx8mp-evk.dtb</code>).</li> </ul> <p>These outputs should be tested on a separate system to confirm correct compilation and structure.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#7-deploying-kernel-artifacts","title":"7. Deploying Kernel Artifacts","text":"<p>Transfer the kernel image and DTBs into the first partition. Maintain appropriate directory layout as referenced by the bootloader. This process may also include updating <code>extlinux.conf</code> if using <code>syslinux</code> or appending boot arguments to match the root filesystem UUID and networking options.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#8-installation-of-the-root-filesystem","title":"8. Installation of the Root Filesystem","text":"<p>Download and extract a minimal root filesystem into the second partition. Arch Linux ARM provides a clean and lightweight base suitable for embedded workloads. Extraction should be performed with <code>bsdtar</code> or <code>tar</code> to preserve ownership and symbolic links:</p> <pre><code>bsdtar -xpf ArchLinuxARM-aarch64-latest.tar.gz -C /mnt/root\n</code></pre> <p>Once extracted, mount necessary pseudo-filesystems (<code>/proc</code>, <code>/sys</code>, <code>/dev</code>) and perform essential post-install steps such as creating fstab entries, locale generation, and user setup.</p> <p>[!NOTE]  Ensure that <code>/etc/fstab</code> uses UUIDs rather than device names for robustness across reboots.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#9-initial-boot-and-diagnostic-monitoring","title":"9. Initial Boot and Diagnostic Monitoring","text":"<p>Unmount all partitions, insert the SD card into the target board, and power on the device. Connect a serial adapter to the board\u2019s UART interface and launch a terminal session:</p> <pre><code># Read target documentation to find the correct port\nsudo minicom -s /dev/ttyUSB3\n</code></pre> <p>Observe bootloader and kernel messages for successful hardware enumeration, rootfs mounting, and init system handoff. Kernel panics or boot stalls at this stage often indicate incorrect DTB or root filesystem misconfiguration.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#10-enabling-wireless-connectivity","title":"10. Enabling Wireless Connectivity","text":"<p>If the onboard Marvell 88W8997 interface is not operational, manually configure it within a <code>chroot</code> environment:</p> <pre><code>mount /dev/sdX2 /mnt\nmount --bind /proc /mnt/proc\nmount --bind /sys /mnt/sys\nmount --bind /dev /mnt/dev\nchroot /mnt\n</code></pre> <p>Within the chroot:</p> <ul> <li>Install firmware (<code>linux-firmware-marvell</code>, <code>mwifiex</code> drivers)</li> <li>Load modules manually using <code>modprobe</code></li> <li>Verify interface status with <code>ip link</code> or <code>iwconfig</code></li> </ul> <p>This setup allows temporary use of the host\u2019s networking stack to bootstrap Wi-Fi connectivity on the target system.</p> <p>[!NOTE] Compiling drivers may need a reference to the kernel using <code>-C</code>, for this, copy the result of the previous kernel compilation on <code>/boot</code> and point to that directory on the compilation. Do not clean the headers of the compilation, or you can delete the references needed.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#11-completing-arch-linux-configuration","title":"11. Completing Arch Linux Configuration","text":"<p>Finalize system configuration:</p> <ul> <li>Set hostname, root password, and create user accounts</li> <li>Generate locales and keymaps</li> <li>Enable networking services (<code>systemd-networkd</code>, <code>wpa_supplicant</code>)</li> <li>Install useful tools such as <code>net-tools</code>, <code>vim</code>, <code>htop</code>, and <code>openssh</code></li> </ul> <p>Optionally, set up SSH key authentication for remote access. Validate internet connectivity prior to proceeding with ECM installation.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#12-ecm-installation-procedure","title":"12. ECM Installation Procedure","text":"<p>Once a working OS environment is established, proceed with installing the ECM suite:</p> <ul> <li>Execute the automated script:</li> </ul> <pre><code>git clone https://github.com/SamthinkGit/GU-systems.git\nsudo ./scripts/autoinstall.sh\n</code></pre> <ul> <li>Alternatively, use the graphical installer provided:</li> </ul> <pre><code>sudo ./installer/install.sh\n</code></pre> <p>Monitor logs to confirm successful dependency installation and service registration.</p> <p>[!WARNING] Avoid using Conda environments in embedded deployments due to their substantial disk and memory requirements. Use <code>pip</code> with virtual environments if necessary.</p>"},{"location":"Bare%E2%80%90Metal-Installation/#13-final-remarks","title":"13. Final Remarks","text":"<p>With the system fully operational and tailored to the constraints of the hardware, you're now equipped to use the ECM's capabilities.</p>"},{"location":"Bibliography/","title":"Bibliography","text":"Alias Name Date Link Keywords PMPA A Survey on Large Language Model based Autonomous Agents 2023 arXiv <code>PMPA</code>, <code>State of Art</code>, <code>Agent Architecture</code> Prompt Engineering A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications 2024 arXiv <code>PromptEngineering</code>, <code>Comparative</code>, <code>Optimization</code> Voyager Voyager: An Open-Ended Embodied Agent with Large Language Models 2023 arXiv <code>AI Model</code>, <code>Minecraft</code>, <code>AutoGPT</code> Fine-Tuned Models vs GPT4 Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks 2023 arXiv <code>Fine-Tuning</code>, <code>PromptEngineering</code>, <code>GPT-4</code> EvalPlus (Code Evaluator) Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation 2023 NeurIPS <code>evalution</code>, <code>test</code>, <code>leaderboards</code> Grammar Prompting Grammar Prompting for Domain-Specific Language Generation with Large Language Models 2023 NeurIPS <code>Formatting</code>, <code>Translation</code>, <code>Specialization</code> LLM Repurposing Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains 2024 arXiv <code>Specialization</code>, <code>Optimization</code>, <code>DSL</code> LLM Planners Understanding the planning of LLM agents: A survey 2024 arXiv <code>Planning</code>, <code>Agents</code>, <code>Autonomous</code> ReAct ReAct: Synergizing Reasoning and Acting in Language Models 2022 arXiv <code>Agent</code>, <code>Planner</code>, <code>Solver</code> Weak/Strong AI Minds, brains, and programs. Behavioral and Brain Sciences 3 (3): 417-45 1980 Web-Archive <code>AI</code> <code>Strong AI</code> <code>Weak AI</code> The Cognition Problem The naturalistic imperative in cognitive science 04/97 BNC <code>Cognition</code> <code>GPS</code> <code>Problem Solving</code> Classical AI Artificial intelligence: a modern approach 2016 Hoasen <code>Classic AI</code> <code>Searching</code> STRIPS STRIPS: A new approach to the application of theorem proving to problem solving 1971 ScienceDirect <code>STRIPS</code> <code>Classic AI</code> PDDL Pddl | the planning domain definition language 1998 ResearchGate <code>PDDL</code> <code>Planning</code> GPT-4 Gpt-4 technical report 2023 ArXiv <code>LLM</code> <code>GPT-4</code> <code>OpenAI</code> Gemini Welcome to the Gemini era: Google DeepMind and the information industry 2023 Emerald <code>Gemini</code> <code>Google</code> <code>LLM</code> Llama2 Llama 2: Open foundation and fine-tuned chat models 2023 ArXiv <code>Llama-2</code> <code>LLM</code> <code>Meta</code> GPS Report on a general problem-solving program. 1959 bitSavers <code>GPS</code> <code>Problem Solver</code> <code>Cognition</code> <code>Classical AI</code> AGI Position: Levels of AGI for Operationalizing Progress on the Path to AGI 2023 ArXiv <code>AGI</code> <code>Modern AI</code> <code>Strong AI</code> AutoGPT From GPT to AutoGPT: a Brief Attention in NLP Processing using DL 2023 ResearchGate <code>AGI</code> <code>Agents</code> <code>Deployment</code> <code>GPT</code> SOAR The Soar Cognitive Architecture 2019 GoogleBooks <code>SOAR</code> <code>Cognition</code> <code>Architecture</code> Explore/Exploit Exploration and exploitation in evolutionary algorithms: A survey 2013 ArXiv <code>Exploration</code> <code>Explotation</code> <code>Search</code> <code>Tree-Of-Thoughts</code> Q* and State-Of-Art From google gemini to openai q*(q-star): A survey of reshaping the generative artificial intelligence (ai) research landscape 2023 ArXiV <code>Q*</code> <code>State-Of-Art</code> <code>Mixture-Of-Experts (MOE)</code> <code>AGI</code>"},{"location":"Cognition-Layer-API-%28Planex%29/","title":"Cognition Layer API (Planex)","text":"<p>[!WARNING] The Cognition Layer API has been replaced by <code>FastAgentProtocol</code> please refer to that documentation for updated docs.</p>"},{"location":"Cognition-Layer-API-%28Planex%29/#cognition-layer-api","title":"Cognition Layer API","text":"<p>The Cognition Layer is the component of the ECM responsible for reasoning, planning, and resolving an Exelent file based on a user's query. User queries can vary widely, encompassing actions such as \"write Hello World in Notepad\" or \"edit this image in Photoshop.\"</p> <p>The ECM does not impose restrictions on the language or framework used to implement the cognition layer. We leverage the Agent Protocol REST API to facilitate communication with agents. In this tutorial, we will use Planex as an example agent for planning.</p> <p>Planex is a 3-step planner agent utilizing LangChain to approximate the ECM problem. This approximation involves three agents focusing on Planning, Reduction, and Translation, based on the Theoretical Analysis. However, the REST API interface remains consistent across all agents.</p>"},{"location":"Cognition-Layer-API-%28Planex%29/#settling-up","title":"Settling up","text":"<p>As Planex operates as an API-based agent, it requires starting the PlanexServer to initialize all necessary components. Building the server is a blocking call, so it can be run in a separate .py file or a different process:</p> <pre><code>import atexit\nimport multiprocessing\nfrom cognition_layer.planexv2.api.server import PlanexV2Server\n\nserver = PlanexV2Server(verbose=True)\nserver_process = multiprocessing.Process(target=server.start)\nserver_process.start()\n\n# We can manually join the proccess, but we will settle it as an atexit task for easing it.    \natexit.register(lambda p: p.join(), server_process)\n</code></pre> <p>Using the <code>verbose</code> argument prints all intermediate steps.</p>"},{"location":"Cognition-Layer-API-%28Planex%29/#running-planex","title":"Running Planex","text":"<p>With everything set up, we can start passing queries to Planex. There are two options for this:</p> <ul> <li>[Recommended] Using the Mediator class of the ECM, which simplifies API calls for Planex (or other Cognition Layer Agents).</li> <li>Directly using the Agent Protocol. Planex has two handlers (task_handler and step_handler) that support all API calls from the AP, allowing free usage if preferred.</li> </ul> <p>In this tutorial, we will use the Mediator class to run Planex. Note that this mediator acts as a client for the previously invoked RestAPI, so it must run within an APIClient from the Agent Protocol. First, we need to configure the API:</p> <pre><code>import agent_protocol_client\nfrom cognition_layer.constants import API_ADDRESS\nfrom cognition_layer.constants import API_PORT\n\nhost = f\"http://{API_ADDRESS}:{API_PORT}\"\nconfiguration = agent_protocol_client.Configuration(host=host)\n</code></pre> <p>In this case, we use the default address and port of the ECM (0.0.0.0:8090), but feel free to change them as needed. Ensure the new address is updated in the ECM constants so the server adapts accordingly.</p> <p>With the configuration set, we can query a task using the mediator. The mediator is an async function and should be called within an async definition.</p> <pre><code>from cognition_layer.api import CognitionMediator\nimport asyncio\n\nasync def main():\n    async with agent_protocol_client.ApiClient(configuration) as api_client:\n\n        mediator = CognitionMediator(api_client)\n        while True:\n\n            query = input(\"Request a Task: \")\n            task = await mediator.get_task(input=query)\n            result = await mediator.run_task(task)\n            print (\"Result: \", result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"ECM-Problem-Analysis/","title":"ECM Problem Analysis","text":""},{"location":"ECM-Problem-Analysis/#definition-of-the-problem","title":"Definition of the Problem","text":"<p>To achieve an ECM (Efficient Computation Model), it is essential to demonstrate that the following equality holds true:</p> <p>$$[ \\text{Alg}_1(p \\mid C') = { \\lambda \\mid \\lambda \\in A' \\text{ and } \\text{Alg}_2(\\lambda)=s' } ]$$</p> <p>Note: Remember that Alg is an alias for Algorithm</p> <p>For a deep and thorough understanding of this concept, we must first define the core variables within this context:</p>"},{"location":"ECM-Problem-Analysis/#1-action-space","title":"1. Action Space:","text":"<p>The action space, denoted as $A$, refers to the comprehensive set of all possible actions abstracted from the framework that can be utilized to solve any given problem. Here, we distinguish three types:</p> <ul> <li>$A$: This denotes all the conceivable actions that can be employed to resolve any problem. In this context, we assume that all problems are solvable using this space.</li> <li>$A^*$: This refers to the minimal set of actions required to solve a specified problem. Thus, all problems can be solved utilizing this space. Unlike $A$, $A^*$ must be computable and Turing-Complete, ensuring that it can be executed algorithmically.</li> <li>$A'$: This represents the set of actions contained within a particular approximation to the ECM problem. In this scenario, not all problems can be solved using this space, highlighting its limitations.</li> </ul>"},{"location":"ECM-Problem-Analysis/#2-action-sequences","title":"2. Action Sequences:","text":"<p>An action sequence, represented by $\\lambda$, refers to a specific series of actions used to address and solve a particular problem. We categorize action sequences into three types:</p> <ul> <li>$\\lambda$: This denotes the perfect set of actions, representing the ideal combination of steps required to solve a given problem most efficiently.</li> <li>$\\lambda^*$: This refers to an optimal set of actions, which is one possible set of steps that can at least solve the problem, though it may not be the most efficient.</li> <li>$\\lambda'$: This signifies a proposed set of actions generated by an algorithm intended to solve a unique problem. This proposed set does not guarantee that the problem will be solved, indicating potential inefficiencies or gaps in the algorithm.</li> </ul>"},{"location":"ECM-Problem-Analysis/#3-solutions","title":"3. Solutions:","text":"<p>A solution, denoted by $s$, is an abstraction representing the resolution of a problem. We define solutions as follows:</p> <ul> <li>$s$: This is a comprehensive solution that is capable of solving all conceivable problems.</li> <li>$s^*$: This denotes a solution that is tailored to solve a specific, given problem.</li> <li>$s'$: This represents a potential solution to a given problem, which is proposed but not assured to be effective or successful.</li> </ul>"},{"location":"ECM-Problem-Analysis/#summary-table","title":"Summary Table","text":"Perfect (All Problems) Optimal (One problem) Approximation (Could Fail) Action Space $$A$$ $$A^*$$ $$A'$$ Sequence $$\\lambda$$ $$\\lambda^*$$ $$\\lambda'$$ Solution $$s$$ $$s^*$$ $$s'$$ <p>Note that we will sometimes use $\\lambda_n$ for referring to a specific instance of $\\lambda$.</p>"},{"location":"ECM-Problem-Analysis/#error-abstraction","title":"Error Abstraction","text":"<p>To design and optimize the problem, we need to compute some form of error to estimate and compare the behavior of the ECM. We analyze the error of the core formula by splitting it into two parts:</p> <ul> <li>Cognition Space: Refers to the planning of the problem, mainly approached using NLP-based Agents.</li> <li>Execution Space: Refers to the execution of a given plan, approached using a more deterministic framework.</li> </ul>"},{"location":"ECM-Problem-Analysis/#cognition-space","title":"Cognition Space","text":"<p>Suppose we have a problem $p$ and we want to find a sequence of actions $\\lambda$ to solve it. The agent faces three main problems:</p> <ul> <li>Planification: Does the agent have the knowledge/capability to solve the problem? The core issue here is the agent's reasoning about the problem and generating an approximation to the problem, independent of the tools available.</li> <li>Reduction: How can the agent execute its plan with the given tools? The core issue here is reducing the computed actions to those the agent can use in its framework. If the framework lacks necessary tools, this step is impossible without failure.</li> <li>Translation: How can the agent translate its reasoning into a concrete framework/language? The core issue here is translating natural language reasoning into a programmatic definition.</li> </ul>"},{"location":"ECM-Problem-Analysis/#execution-space","title":"Execution Space","text":"<p>After planning and reasoning, the steps must be executed to reach a solution. Similar to following a recipe, execution might not go as intended:</p> <ul> <li>Interpretation: There could be differences between the specified plan and runtime execution. For example, the plan might specify \"click the first word containing 'A'\", which could result in varied interpretations.</li> <li>Execution: Even ignoring interpretation, the planned step might raise an unforeseen exception, requiring the agent to re-plan.</li> </ul> <p>After having defined each type of error we want to make an abstraction of those errors, so we can analyze how we can make an approximation of them and know what should be improved. Let's see step by step an abstraction of each one:</p>"},{"location":"ECM-Problem-Analysis/#planification","title":"Planification","text":"<p>$$ Planificate(query) \\rightarrow  \\lambda_1 := [a,b,c,...] \\subseteq A \\mid Alg_2(\\lambda_1) = s $$</p> <p>Let's break this equation down: The planification step is a function that receives a NL-query $Planificate(query)$ and returns a set of actions $[a,b,c,...]$ referred to as $\\lambda_1$. This function must meet two conditions: First, $\\lambda_1$ should be a subset of actions in $A$. Second, these actions should be executable to solve the query $Alg_2(\\lambda_1) = s$.</p> <p>Error:</p> <p>$$error_{plan} = 0 + [s - Alg_2(\\lambda_1)] = Alg_2(\\lambda) - Alg_2(\\lambda_1)$$</p> <p>For computing the error, we can split the first equation into two parts. First, note that the possible error in $\\lambda_1 := [a,b,c,...] \\subseteq A$ is $0$ because:</p> <p>$A$ denotes all the conceivable actions that can be employed to resolve any problem.</p> <p>So by definition, the set of actions must be a subset of $A$. Then, we can define the error of $Alg_2(\\lambda_1)$ by comparing it with its expected value $s$. If the algorithm should return $s$, the error is $s - Alg_2(\\lambda_1)$.  Note that a solution $s$ can also be defined as the execution of a perfect plan, $Alg_2(\\lambda)$, where $\\lambda$ is the perfect plan, leading to:</p> <p>$$error_{plan} = Alg_2(\\lambda) - Alg_2(\\lambda_1)$$</p>"},{"location":"ECM-Problem-Analysis/#reduction","title":"Reduction","text":"<p>$$Reduce(\\lambda_1) \\rightarrow \\lambda_2 := [x,y,...] \\subseteq A^* \\mid Alg_2(\\lambda_2) = s^*$$</p> <p>The reduction error is a function that receives a plan (a sequence of NL-actions) and reduces that plan to one where all actions are a subset of $A^*$. Note how we must consider the same conditions as in the planification section.</p> <p>Error:</p> <p>$$error_{reduction} = [ \\lambda^* - \\lambda_2 ] \\cup [ Alg_2(\\lambda^*) - Alg_2(\\lambda_2) ]$$</p> <p>Following the same reasoning, we split the equation into two parts. First, there could be a gap between the optimal plan $\\lambda^*$ and the generated plan $\\lambda_2$, possibly due to a poor selection of tools even if planification succeeded. Second, there is the same error as previously: the plan must be executable to be valid. Since the optimal plan $\\lambda^*$ is executable, the error of the second part is $Alg_2(\\lambda^*) - Alg_2(\\lambda_2)$.</p> <p>Note that the second condition uses $\\lambda^*$ instead of $\\lambda$ used in the planification step. This is due to the action space $A^*$, where the best solution in an optimal action space is $\\lambda^*$ (not the perfect solution $\\lambda$). Also, we use $\\cup$ notation for joining the errors, since the relationship is not clear.</p>"},{"location":"ECM-Problem-Analysis/#translation","title":"Translation","text":"<p>$$Translate(\\lambda_2) \\rightarrow \\lambda_3$$</p> <p>The translation step is a function that receives the reduced plan and generates an equivalent description in the framework that will use that plan. The error in this process arises from the quality loss of the original plan.</p> <p>Error: $$error_{translation} = \\lambda_2 - \\lambda_3$$</p> <p>Defining the error here is straightforward (theoretically). We compare the plan specified in natural language with its programmatic description. If the match is exact, the error is $0$.</p>"},{"location":"ECM-Problem-Analysis/#interpretation","title":"Interpretation","text":"<p>$$Interpretate(\\lambda_3) \\rightarrow \\lambda_4 := [x_1, y_1, ...] \\subseteq A^*$$</p> <p>The interpretation step is the reverse of the translation step. It can be seen as an encoding-decoding process, where errors arise from recovering the encoded data without quality loss.</p> <p>Error:</p> <p>$$error_{interpretation} = \\lambda_3 - \\lambda_4$$</p>"},{"location":"ECM-Problem-Analysis/#execution","title":"Execution","text":"<p>$$Execute(\\lambda_4) \\rightarrow s_4$$</p> <p>In the execution function, the next step is not defined, as execution itself is the solution. The error is defined by the framework's capacity to properly run the given steps.</p> <p>Error:</p> <p>$$error_{execution} = s_4 - \\lambda_4$$</p>"},{"location":"ECM-Problem-Analysis/#summary-table_1","title":"Summary Table","text":"Step Error Planification $$Alg_2(\\lambda) - Alg_2(\\lambda_1)$$ Reduction $$( \\lambda^* - \\lambda_2 ) \\cup ( Alg_2(\\lambda^*) - Alg_2(\\lambda_2) )$$ Translation (Encoding) $$\\lambda_2 - \\lambda_3$$ Interpretation (Decoding) $$\\lambda_3 - \\lambda_4$$ Execution $$s_4 - \\lambda_4$$"},{"location":"Exelent/","title":"Exelent","text":""},{"location":"Exelent/#introduction","title":"Introduction","text":"<p>So far, multiple planning languages have been developed to be computable, with PDDL (Planning Domain Definition Language) being widely used for automatic plan computation. JSON descriptions have also been utilized to enable LLMs to define objects or generations. In this context, Exelent (an acronym for 'Execution Language') emerges as a faster and easier declarative language with Pythonic syntax. Designed for describing AI-generated plans and sequences of programmatic steps to reach a solution, Exelent combines simplicity with the familiarity of Python.</p>"},{"location":"Exelent/#key-properties-of-exelent","title":"Key Properties of Exelent","text":"<ul> <li> <p>Pythonic Syntax: Exelent uses a Pythonic syntax, making it accessible and familiar to users of Python. This design choice leverages the extensive Python code training datasets of many LLMs, potentially improving results and reducing the need for extensive fine-</p> </li> <li> <p>Efficiency: The minimal use of keywords and dynamic resolution reduces the number of tokens that need to be generated by LLMs, resulting in more efficient and cost-effective requests.</p> </li> <li> <p>Compatibility: Exelent is compatible with all Python functions, including support for multitasking and parallel planning. Additional behaviors are under development to further enhance its capabilities.</p> </li> <li> <p>Ease of Parsing and Extension: Exelent is easy to parse and extend using the built-in ExelentParser or directly with the ast library, which is fully supported.</p> </li> </ul> <p>By incorporating these properties, Exelent aims to provide a robust and efficient solution for AI-driven plan generation and execution.</p>"},{"location":"Exelent/#syntax","title":"Syntax","text":"<p>The syntax of Exelent is straightforward. Each file contains a set of plans, which can be built following three </p> <ol> <li> <p>Define a Plan Function: All plans are defined as functions. A plan definition can include arguments specifying the properties of the plan.</p> </li> <li> <p>Define Types within Each Plan: A type represents a predefined behavior, similar to using loops (e.g., \"for\" or \"while\") in imperative programming languages. Types can have properties specified through its arguments. </p> </li> <li> <p>Define Actions within Each Type: An action corresponds to a function that will be linked during the interpretation of the file. Actions are similar to function calls but without declarations, imports, or definitions.</p> </li> </ol> <p>Note: This section describes the syntax and structures of the Exelent language. However, it is essential to check the functionality support in the Interpreter Capabilities Table for the interpreter you select. We are working on developing a stable and unified interpreter for Exelent.</p> <p>Following these steps, a standard plan in Exelent will have the following structure:</p> <pre><code>def &lt;plan&gt;(&lt;properties&gt;):\n  with &lt;type&gt;(&lt;properties&gt;):\n    &lt;action&gt;()\n    &lt;action&gt;()\n</code></pre> <p>As an example, this is a hello_world.xlnt file:</p> <pre><code># hello_world.xlnt\n\ndef hello_world():\n  with Sequential():\n    print(\"Hello World!\")\n</code></pre>"},{"location":"Exelent/#rules-and-specifications","title":"Rules and Specifications","text":"<p>Although Exelent look like standard Python programs (and are parsed as such), they must adhere to the following specifications to conform to Exelent:</p> <ol> <li>Rule of Non-Definition: Exelent files do not contain \"import\" statements, function definitions, or imperative keywords like loops or conditionals. This is because Exelent is a declarative language designed to look like Python. The interpreter is responsible for linking and resolving all functions used in the file. For example:</li> </ol> <pre><code># The file does not contain any more\n\ndef complex_task():\n  with Sequential():\n    do_task_1(\"arg1\")\n    do_task_2(arg2=\"arg2\")\n</code></pre> <p>Note: While having files with undefined functions is unusual in other languages, in this time, the interpreter will resolve these functions automaticall, so there is no need to worry.</p> <ol> <li> <p>KeywordCase Rule: Python reserved keywords remain unchanged, but all Exelent-specific keywords start with an uppercase letter. For instance, \"Sequential()\" is a reserved type in Exelent, so all function/action names must start with a lowercase letter.</p> </li> <li> <p>Pythonic Syntax: All code must be indented and follow a syntax compatible with Python (version 3.10 or later), including proper tabulation, allowing for easier argument formatting.</p> </li> </ol>"},{"location":"Exelent/#types","title":"Types","text":"<p>Each with statement in Exelent is associated with a predefined type. The built-in types in Exelent, which must be supported by each interpreter, include:</p> <ul> <li><code>Sequential</code>: Executes the following actions in order. If an action fails, it exits and returns an error.</li> <li><code>Parallel</code>: Executes the following actions concurrently, initializing and resolving their feedback simultaneously.</li> </ul>"},{"location":"Exelent/#conclusion","title":"Conclusion","text":"<p>You have now learned the fundamentals of using Exelent to define and execute plans with Pythonic syntax. By mastering the straightforward rules and leveraging predefined types, you are well-equipped to create efficient and flexible plans that seamlessly integrate with Python functions using Exelent. You can take a look to the interpreter tutorials to start using Exelent. </p>"},{"location":"FastAgentProtocol/","title":"FastAgentProtocol Documentation","text":"<p>The <code>FastAgentProtocol</code> is designed to provide a simplified, faster alternative to the previous <code>AgentProtocol</code>. It enables efficient communication between cognitive agents and the main system without the overhead of remote connections.</p>"},{"location":"FastAgentProtocol/#why-use-fastagentprotocol","title":"Why Use FastAgentProtocol?","text":"<ul> <li>Speed &amp; Simplicity: It removes the need for remote server connections, significantly reducing latency.</li> <li>Standardization: All agents in the cognition layer must implement this protocol for consistent communication.</li> <li>Flexibility: The main system no longer needs to manage execution, as the agent handles it independently.</li> </ul>"},{"location":"FastAgentProtocol/#how-it-works","title":"How It Works","text":"<p>The <code>FastAgentProtocol</code> requires defining how an agent processes tasks in steps, where each step returns essential feedback to the main system.</p>"},{"location":"FastAgentProtocol/#key-components","title":"Key Components","text":"<ul> <li>Iterator: A function that processes input iteratively, yielding steps.</li> <li>Content Getter: Retrieves the content of each step.</li> <li>Is Last Getter: Determines if the current step is the final one.</li> <li>Step Name Getter (optional): Assigns a name to each step.</li> </ul>"},{"location":"FastAgentProtocol/#implementing-a-custom-agent-with-fastagentprotocol","title":"Implementing a Custom Agent with FastAgentProtocol","text":"<ol> <li>Define an Iterative Process: The agent's main function should yield steps, each containing a name, content, and finalization status.</li> <li>Create the Protocol Server:<ul> <li>Use the <code>FastAgentProtocol</code> class.</li> <li>Define getters for step information.</li> </ul> </li> <li>Example Custom Agent</li> </ol> <pre><code>from cognition_layer.api import FastAgentProtocol\nfrom ecm.mediator.Interpreter import Interpreter\nfrom operator import attrgetter\n\nclass MyAgent:\n    def __init__(self, interpreter: Interpreter):\n        self.interpreter = interpreter\n\n    def complete_task(self, input: str):\n\n        my_agent.send_user_input(input)\n        for step in steps:\n            next_action = my_agent.next_action()\n            result = interpreter.run(next_action)\n            step = parse_result_contents(result)\n            yield step\n\ndef get_fast_ap_server(interpreter: Interpreter) -&gt; FastAgentProtocol:\n    agent = MyAgent(interpreter)\n\n    return FastAgentProtocol(\n        name=\"My Custom Agent\",\n        iterator=agent.complete_task,\n        step_name_getter=attrgetter(\"name\"),    # Equivalent to step.name\n        content_getter=attrgetter(\"content\"),   # Equivalent to step.content\n        is_last_getter=attrgetter(\"is_last\"),   # Equivalent to step.is_last\n    )\n</code></pre>"},{"location":"FastAgentProtocol/#using-an-implemented-serverwith-fast-ap","title":"Using an implemented serverwith Fast AP","text":"<p>To use an agent with FastAgentProtocol in the main program:</p> <pre><code>from cognition_layer.fast_react.api.server import get_fast_ap_server\nfrom cognition_layer.tools.registry import ItemRegistry\n\nserver = get_fast_ap_server(interpreter=interpreter)\n\nwhile True:\n    query = input(\"Task: \")\n    for step in server.send_task(query):\n        print(f\"Step: {step.name} -&gt; {step.content}\")\n</code></pre>"},{"location":"FastAgentProtocol/#conclusion","title":"Conclusion","text":"<p><code>FastAgentProtocol</code> provides a unified, efficient approach to agent communication. By following the pattern of defining iterators and feedback structures, agents can seamlessly integrate with the main system while optimizing performance and reducing complexity.</p>"},{"location":"FastReact/","title":"FastReact","text":"<p>FastReact is a cognitive agent designed to optimize the reasoning and action process using the ReAct methodology (Reason + Act). Its primary goal is to accelerate task execution by combining reasoning and action in a single iteration, reducing latency while maintaining system efficiency.</p>"},{"location":"FastReact/#motivation-and-approach","title":"Motivation and Approach","text":"<p>The motivation behind FastReact arises from the limitations of previous models like Xplore, which, while robust, were slow due to the separation between reasoning and action execution. FastReact overcomes this by ensuring that each iteration performs both processes in a single call to the LLM. This is achieved by training the LLM to always respond in a structured JSON format, where it is forced to reason and generate the Python code that represents the action to be executed.</p>"},{"location":"FastReact/#fastreact-workflow","title":"FastReact Workflow","text":"<p>The workflow of FastReact follows these primary steps:</p> <ol> <li>Initialization: Cognitive memory is configured with key instructions, and initial messages are preserved to establish the task's starting state.</li> <li>Capturing the Current State: In each iteration, a screenshot of the system is taken and converted into a message that is added to the history, providing the LLM with a visual understanding of the current context.</li> <li>Generating the Response: The LLM receives the full interaction history and produces a structured JSON.</li> <li>Execution: The Python code is parsed using the Exelent language and executed through the system's interpreter.</li> <li>Memory Update: The LLM's response is saved into the cognitive memory to ensure continuity and progress in future iterations.</li> </ol> <p>This process repeats until the LLM indicates that the task has been completed. Cognitive memory plays an essential role, preserving critical instructions and optimizing token usage by removing unnecessary images, thereby reducing computational costs.</p> <p>This mechanism can be obtained by using the following prompt:</p> <pre><code>class JsonFastReactResponse(BaseModel):\n    reasoning: str = Field(description=\"A reasoning for solving the task\")\n    function: str = Field(\n        description=\"The function with pythonic notation. E.g: myfunc(2, 3, 'foo')\"\n    )\n    all_tasks_completed: bool = Field(\n        description=\"True if and only if all tasks from the user have been completed\"\n    )\n\nFR_PROMPT = f\"\"\"\nYou are a ReAct agent. On each call you must observe the history and then generate the appropriate reasoning + action in json format.\n\nThe response should use the following json template:\n{FastReactParser.get_format_instructions()}\n\nEnsure the arguments use pythonic notation.\nThe system will call you with the observation from your action.\n\"\"\"\n\n</code></pre>"},{"location":"FastReact/#conclusion","title":"Conclusion","text":"<p>FastReact represents an evolution in cognitive agent design respect to other agents such as Xpore or RePlan. By consolidating reasoning and action into a single call and automating execution through Exelent, it offers a fast, efficient, and adaptable solution. Its integration of cognitive memory and visual perception through screenshots makes it a robust and agile agent, capable of handling complex tasks in changing environments.</p>"},{"location":"Home/","title":"\ud83c\udf1f GU-Systems: An ECM approximation to AGI\ud83c\udf1f","text":""},{"location":"Home/#introduction","title":"\ud83d\ude80 Introduction","text":"<p>Welcome to the main repository for our cutting-edge project on the implementation of ECM (Execution-Cognition Machine) as a strategic approach towards the development of Computer Autonomous AI. This initiative represents a significant leap in crafting an intelligent system that not only mimic but also evolve human-like cognitive functionalities autonomously in computer interaction.</p> <p>\ud83e\udde0 The project revolves around designing, testing, and refining theoretical models that translate complex human problem-solving capabilities into computational strategies executable by AI. We integrate advanced AI frameworks. Our system architecture is robust, accommodating rapid iterations and rigorous validations through continuous integration and deployment mechanisms.</p> <p>\ud83d\udcda For detailed insights into our methodologies and system functionalities, please refer to the README, the documentation included in this repository or surf through the pages of this Wiki.</p>"},{"location":"Installation-Guide/","title":"Installation Guide","text":"<p>First, clone the repository to your machine:</p> <p>[!NOTE] It is recommended to use a Conda environment for this project. If you choose to do so, install Conda beforehand.</p> <pre><code>git clone https://github.com/SamthinkGit/GU-systems.git\ncd GU-systems\n</code></pre>"},{"location":"Installation-Guide/#linux","title":"Linux","text":"<p>If you are using Linux, run the auto-installation script:</p> <pre><code>./installer/install.sh\n</code></pre> <p>[!WARNING] The Linux installation only supports server mode. In the current version, action space is only ensured to be stable for Windows installations.</p>"},{"location":"Installation-Guide/#windows","title":"Windows","text":"<p>If you are using Windows, open the file <code>&lt;repo_directory&gt;/installer/install.bat</code> with admin permissions.</p> <p>You can double-click on the file and select Run as Administrator.</p> <p>Once launched, wait a few minutes. An interface will be automatically installed to guide you through the rest of the installation. After a few minutes, a message will appear with an IP address you can access to complete the setup. If the browser does not open automatically, please open it manually and enter the provided IP.</p> <p>Once in the interface, there are two installation modes: Standard Installation and Developer Installation.</p>"},{"location":"Installation-Guide/#standard-installation","title":"Standard Installation","text":"<ol> <li> <p>You must choose an Execution Layer and a Cognition Layer. Usually, the versions labeled <code>Latest</code> are recommended, and typically do not require any change.</p> </li> <li> <p>There is an option to install with Conda, but this will only work if you have previously installed Conda or Miniconda. The default environment will be <code>ecm</code>, custom environments must be created through the developer tab.</p> </li> <li> <p>You will find three input fields to insert the API keys required by the ECM. These keys are necessary for the latest versions and must be obtained in advance, either with administrator approval or through the following pages:</p> </li> <li> <p>OpenAI API Key</p> </li> <li>Replicate API Key</li> <li>Moondream API Key</li> </ol> <p>When accessing these pages, you will be prompted to create an account if you don't already have one. Once logged in, you will be able to generate and copy the necessary token.</p> <ol> <li>After filling in the keys, click on Install. A confirmation window will appear and the installation will proceed automatically.</li> </ol>"},{"location":"Installation-Guide/#developer-installation","title":"Developer Installation","text":"<p>This mode offers more options and can be used to perform partial installations or update current installations to newer (including experimental) versions. From here, you can also install the host/client mode, where the cognition layer and execution layer can be separated.</p> <ol> <li> <p>After clicking the Developer button, wait about one minute while it connects to a shell and gathers data from the console to help display the current installation status.</p> </li> <li> <p>If Conda is installed:</p> </li> <li>You can change the current environment using the dropdown menu labeled Change conda environment:</li> <li>The currently active environment will be displayed next to: Current active conda environment</li> <li>In case of errors, please check your Conda installation and click Reload</li> <li> <p>If Conda is installed but you wish to install directly with Python, select the environment None</p> </li> <li> <p>Multiple installable package options will appear for the ECM. If a package is marked as Installed, all its dependencies have been satisfied.</p> </li> </ol> <p>[!NOTE] It is recommended to install at minimum the <code>Base</code> and <code>Devel</code> dependencies.</p>"},{"location":"Installation-Guide/#options","title":"Options:","text":"<ul> <li> <p><code>Add repository to PYTHONPATH</code>: Adds the repo to the PYTHONPATH so it can be used from any directory. This is used instead of <code>pip install</code> for scalability, allowing the repository to remain editable while installed, and for backwards compatibility.</p> </li> <li> <p><code>Install Precommit tools (Needed for pushing to GitHub)</code>: Required to commit/push to the GitHub repo. It sets up pre-commit checks before each commit.</p> </li> <li> <p><code>Pull to latest version (experimental)</code>: Updates the repository, including external ones. It is recommended to always enable this to stay on the latest stable version.</p> </li> <li> <p>API keys work the same as described in the Standard Installation.</p> </li> </ul> <p>[!TIP] During the installation, all executed commands are visible from the terminal initially launched. In case of errors, please check the logs there and upload them when creating an issue.</p> <p>After reviewing everything, click <code>Install</code>. For the usage guide after installation, please refer to the <code>Usage</code> entry in this wiki.</p>"},{"location":"Interpreter/","title":"Interpreter","text":""},{"location":"Interpreter/#introduction","title":"Introduction","text":"<p>The Interpreter module provides a framework for executing Exelent files. It acts as an abstract class to unify various execution layer managers. This guide will demonstrate how to use the interpreter with ROSA as an example interpreter, the same applies for later implementations such as Pyxcel.</p>"},{"location":"Interpreter/#functionalities-overview","title":"Functionalities Overview","text":"<p>All interpreters are located in the ecm/mediator directory. For this example, we will import the RosaInterpreter as follows:</p> <pre><code>from ecm.mediator.rosa_interpreter import RosaInterpreter, RosaInterpreterSupports\n</code></pre> <p>Each interpreter defines a support class that specifies its capabilities. You can visualize the supported properties with the following command:</p> <pre><code>print(RosaInterpreterSupports())\n</code></pre> <p>The core functionality of interpreters is provided by the methods they implement. While interpreters can define additional methods, the predefined ones include:</p> <ul> <li><code>run</code>: Receives a ParsedTask object and executes it from start to finish. It may also accept a callback argument to define a function for receiving execution feedback.</li> <li><code>arun</code>: Receives a ParsedTask object and executes it asynchronously, accepting a callback argument for feedback.</li> <li><code>stop</code>: Stops a running task by its name.</li> <li><code>hard_stop</code>: Forcefully stops a running task by its name (use with caution as it may have undesirable effects).</li> <li><code>wait_for</code>: Waits for a specific call from the task, such as a status change or task completion.</li> <li><code>kill</code>: Closes the interpreter (does not ensure it is capable to be used again in the same process)</li> </ul>"},{"location":"Interpreter/#using-the-interpreter","title":"Using the Interpreter","text":"<p>To use the RosaInterpreter, first initialize it:</p> <pre><code>from ecm.mediator.rosa_interpreter import RosaInterpreter\ninterpreter = RosaInterpreter()\n</code></pre> <p>To execute a task, use the <code>run</code> method. You need to parse an Exelent file with the actions to execute. Optionally, you can provide a callback function to handle task feedback. The callback implementation specifics are documented by the selected interpreter. For this example, we will use the hello_world file found in /tests/resources/hello_world.xlnt.</p> <pre><code># This is the .xlnt file\ndef my_task():\n    with Sequential(on_fail=DUMP_LOGS):\n        with Sequential():\n            click(\"windows_icon\")\n            click(\"notepad\")\n\n        with Sequential():\n            click(\"window\")\n            write(\"Hello World!\")\n</code></pre> <p>This file defines two actions: click and write. You need to implement these actions before executing the task:</p> <pre><code>from ecm.tools.registry import ItemRegistry\n\n# Note that just by registering the function will make it available for all interpreters.\n@ItemRegistry.register_function\ndef click(location: str):\n    print(f\"Clicked {location}!\")\n\n\n@ItemRegistry.register_function\ndef write(text: str):\n    print(text)\n</code></pre> <p>With the Exelent file and the tools set up, you are ready to execute the task:</p> <pre><code>import ecm.exelent.parser as parser\nfrom ecm.shared import get_root_path\n\npath = get_root_path() / \"tests\" / \"resources\" / \"hello_world.xlnt\"\ntask = parser.parse(path)\ninterpreter.run(task, callback=\"silent\")\n</code></pre> <p>In this example, we pass the \"silent\" keyword to the callback. Each interpreter may support different keywords or objects.</p> <pre><code>&gt;&gt;&gt; interpreter.run(task, callback=\"silent\")\nClicked windows_icon!\nClicked notepad!\nClicked window!\nHello World!\n</code></pre> <p>After you have finished using the interpreter, some implementations require cleanup. Therefore, it is good practice to terminate the interpreter.</p> <pre><code>interpreter.kill()\n</code></pre>"},{"location":"Interpreter/#conclusion","title":"Conclusion","text":"<p>By following this guide, you now have a foundational understanding of how to utilize the Interpreter module to execute tasks parsed by the Exelent Parser. This knowledge will allow you to manage and control task execution within your projects effectively. Remember to clean up by terminating the interpreter when it's no longer needed to maintain system integrity. Happy coding!</p>"},{"location":"ItemRegistry/","title":"Introduction","text":"<p>The ItemRegistry is a class designed similar to a database. Its main purpose is to store what we call Actions, Tools and Items in order to enable modules such as the Exelent Parser or the Interpreters to retrieve complex objects by just using its name (an string). In the following sections, you will learn how to use, interact and register your own items inside the ItemRegistry and some deprecations that you can still found in some directories.</p>"},{"location":"ItemRegistry/#initializing","title":"Initializing","text":"<p>The ItemRegistry acts as a database, thus it is able to be shared between different layers and threads even when you don't have access to the same instance. For obtaining this, you can use 2 different types of instantiation:</p> <ol> <li>If you call the ItemRegistry without arguments, you will receive the default ItemRegistry, all the threads will share the same default instance.</li> </ol> <pre><code>from ecm.tools.item_registry_v2 import ItemRegistry\n\nobj_1 = object()\nobj_2 = object()\n\nregistry_1 = ItemRegistry()\nregistry_2 = ItemRegistry()\n\nprint(obj_1 is obj_2) # -&gt; False\nprint(registry_1 is registry_2) # -&gt; True\n</code></pre> <ol> <li>If you want to force the registry to maintain a separated database, you can add a name to you registry, this will return an empty registry that can be called from other threads only if they use the same name:</li> </ol> <pre><code>from ecm.tools.item_registry_v2 import ItemRegistry\n\ndefault_registry = ItemRegistry()\nmy_registry = ItemRegistry(name=\"different\")\nmy_registry_2 = ItemRegistry(name=\"different\")\n\nprint(default_registry is my_registry) # -&gt; False\nprint(my_registry is my_registry_2) # -&gt; True\n</code></pre> <p>For obtaining information about the current instances and its content you can use 2 funcions:</p> <pre><code>ItemRegistry.summary()\n# We add some items...\n</code></pre> <pre><code>==================== Item Registry ====================\n[default]\n* (ACTION): Action(name='click', active=True, content=&lt;function click at 0x7607e0f7fa30&gt;, is_callable=True, description=\"Clicks with the mouse on the specified element. Example: click('Firefox Icon') or click('Navigation Bar'). Ensure to provide an especific description if needed.\", labels=[])\n* (TOOL): Tool(name='screenshot', active=True, content=&lt;function screenshot at 0x7607e79c8af0&gt;, is_callable=True, description='', labels=[])\n[different]\n* (TOOL): Tool(name='move_mouse_to', active=True, content=&lt;function move_mouse_to at 0x760800cee0e0&gt;, is_callable=True, description='', labels=[])\n* (TOOL): Tool(name='send_click_event', active=True, content=&lt;function send_click_event at 0x7607e0f7f9a0&gt;, is_callable=True, description='', labels=[])\n</code></pre>"},{"location":"ItemRegistry/#adding-actions-and-tools","title":"Adding Actions and Tools","text":"<p>When working with agents we will want the AI to retrieve the function <code>use_my_mouse()</code> with only requesting to the agent a string that says: <code>\"use_my_mouse\"</code>. For this, since only some functions are targeted to be used by an AI, you must register your functions in the ItemRegistry.</p> <p>Here it is important to differerentiate between two types of functions: - Actions: These are those functions targeted to the AI, they should contain a docstring, be fully typed and clear and be simple to use. This means, you should at most receive one or two arguments (if any) and the use should be self-explanatory. Use arguments as <code>int</code> or <code>str</code> for best practices. A good action could be the following:</p> <pre><code>@ItemRegistry.register(type=\"action\")\ndef close_app(name_of_the_app: str) -&gt; None:\n  \"\"\"Closes an app by its name. The app should be active to be closed. Usage: close(\"firefox\").\"\"\"\n  app = my_app_retriever.retrieve(name_of_the_app)\n  app.close()\n  ...\n</code></pre> <ul> <li>Tools: These are functions that are stored in the ItemRegistry are not targeted to the AI but the developer. This can be defined as any other function.</li> </ul> <pre><code>@ItemRegistry.register(type=\"tool\")\ndef close_app(app_name: str, app_registry: AppRegistry, check_for_errors: bool = True) -&gt; None:\n  \"\"\"\n  Generates a summary of all the current apps\n  \"\"\"\n  ...\n</code></pre> <p>As you can see, all the actions and tools are registered by just using the <code>@ItemRegistry().register</code> decorator. Note that this decorator will be executed as the same time as the file is imported, for this reason, you will see multiple imports of actions when the developer wants the actions to be available. These functions should be saved in the /action_space directory, usually in a file called as <code>actions.py</code>.</p> <pre><code># We use the `noqa` label to avoid intellisense or formatters to flag this import as an error\nimport action_space.experimental.screenshot.actions # noqa\n</code></pre>"},{"location":"ItemRegistry/#loading-actions","title":"Loading Actions","text":"<p>Following the Principle of Least Atonishment in python the imports that are used to save the actions/tools, are not fully loaded into the registry, so imports don't alter the functioning of your program. After you have selected which actions you have imported, they must be loaded in the ItemRegistry instance you want to work with.</p> <p>For example, you can use the <code>load_all()</code> method to enable all tools and actions retrieved in the imports into a selected instance:</p> <pre><code>from ecm.tools.item_registry_v2 import ItemRegistry\nimport action_space.my_lib.actions # noqa\n\n# No action avaliable\nItemRegistry.summary()\n\n# Note that we are calling the default instance\nItemRegistry().load_all()\n\n# All actions are available\nItemRegistry.summary()\n</code></pre> <p>If you want to load only a specific action, you can use the <code>load()</code> method:</p> <pre><code>ItemRegistry().load(\"screenshot\")\n</code></pre> <p>Sometimes you will want to add multiple actions that are related, for example, all the keyboard utils. For this, the version V2 of the ItemRegistry enables the actions to be grouped inside a package. Then you can easily load them all at the same time.</p> <pre><code>from ecm.tools.item_registry_v2 import ItemRegistry\n@ItemRegistry.register(package=\"keyboard\")\ndef press_A():\n  \"\"\"I press the Key A\"\"\"\n  ...\n@ItemRegistry.register(package=\"keyboard\")\ndef press_B():\n  \"\"\"I press the Key A\"\"\"\n  ...\n\nItemRegistry().load_package(\"keyboard\")\n</code></pre> <p>Note: Some functions in this repository are still using the version V1 of the ItemRegistry to maintain backward-compatibility and thus, cannot be encapsulated into a package.</p>"},{"location":"ItemRegistry/#extracting-actions-and-tools","title":"Extracting Actions and Tools","text":"<p>Now that you know how to integrate actions and tools, you can exit here if you only are going to use these registry for building new actions or tools to AI's (all the agents extract their capacities from this registry). However, if you are building your own agent, you will need to know how to manage the actions and tools saved in the ItemRegistry.</p> <p>All the functions that are labeled as actions, will be saved in the following dataclass:</p> <pre><code>@dataclass\nclass Action(Item):\n    name: str\n    content: Callable\n    description: str\n    is_callable: bool = True\n</code></pre> <p>As you can see, the name of the function, callable itself and a description are mandatory, and will be automatically retrieved when using <code>ItemRegistry.register</code> from the name of the function and docstring.</p> <p>In the other hand, the Tools have the following dataclass:</p> <pre><code>class Tool(Item):\n    name: str\n    content: Callable\n    is_callable: bool = True\n</code></pre> <p>Similar to the actions the contain a callable, but they not enforce a docstring.</p> <p>Note that if you want to add some metadata to the tools or actions you can use the <code>.label</code>property inherited from the <code>Item</code> class. For example, if you want to mark the tool as dangerous you could add:</p> <pre><code># .label is a list[str]\n\nitem = Item(\"fire\", do_fire)\nitem.label.append(\"danger\")\n</code></pre> <p>All the information about this classes can be found at the item_registry_v2.py file.</p> <p>Now if you want to extract the actions or tools that have been loaded into a registry (for example using <code>load_all()</code>) you can access the <code>.tools</code> or <code>.actions</code> dictionaries, that will return the items by giving their name:</p> <pre><code>registry = ItemRegistry()\n\nall_actions_loaded = registry.actions.values()     # -&gt; list[Action]\nall_tools_loaded = registry.tools.values()         # -&gt; list[Tool]\n\nscreenshot = registry.actions[\"screenshot\"]\nscreenshot()\n\npress_A = registry.actions[\"keyboard.press_A\"]\npress_A\n</code></pre> <p>Note: The actions saved inside a package will be accessible by using their package name followed with a dot. This enables to have multiple actions with the same name, however, is generally not a good idea to use them at the same time, since the AI could missunderstand the difference between each one</p>"},{"location":"ItemRegistry/#some-addons","title":"Some Addons","text":"<p>When developing using the ItemRegistry, we enable to use some tools that will ease the debug of the actions:</p> <ul> <li>Invalidation: By invalidating a registry, all the actions and tools saved will be replaced with a mock that won't execute the function itself but a logging method that will act as the function. For example you could use this to simulate the AI could call a function that shut downs the pc:</li> </ul> <pre><code>registry = ItemRegistry()\nregistry.invalidate()\n\nmock = registry.actions[\"shutdown\"]\nmock(\"now\", force=True)\n</code></pre> <pre><code># Output\n-&gt; mock([now], {force: True})\n</code></pre> <ul> <li> <p>Remote Execution: The ItemRegistry can be configured to be used on a remote machine, obtain more infomation at the RemoteExecution Guide</p> </li> <li> <p>Alias: You can add multiple names to the same function, this is useful for AI when it hallucinates different names for an specific function. Inside the ItemRegistry, they will be saved as different functions:</p> </li> </ul> <pre><code>\n@ItemRegistry.alias([\n  \"type\",\n  \"write_text\",\n])\n@ItemRegistry.register(type=\"action\")\ndef write(text: str):\n  \"\"\"Explanation...\"\"\"\n  ...\n</code></pre> <p>Note: Alias only works for action not tools.</p> <p>This could enlarge the size of your prompt (thera are more actions to be explained), add at most 1 or 2 aliases.</p>"},{"location":"ItemRegistry/#conclusion","title":"Conclusion","text":"<p>With this you have fully learned how to use the ItemRegistry, this class is used in multiple layers over the system such as interpreters, exelent parsers, agents...</p> <p>You can try to add your own actions and register them to the default ItemRegistry, they should be automatically used in the agents integrated in this repo.</p> <p>Note: The deprecated version ItemRegistryV1 can be still found at /ecm/tools/item_registry_v1.py. The file /ecm/tools/registry.py will automatilcally redirect to the v2 registry. This class has been intended to be backward-compatible but the previus versions will not be continuated. If you want to disable the enforcing of the version v2, you can change the flag <code>PATCH_ITEM_REGISTRY_V1</code> at the constats.py file.</p>"},{"location":"LLMs-and-State-of-Art/","title":"LLMs and State of Art","text":""},{"location":"LLMs-and-State-of-Art/#introduction-to-large-language-models-llms","title":"Introduction to Large Language Models (LLMs)","text":"<p>Large Language Models (LLMs) such as OpenAI's GPT series (GPT-3.5, GPT-4, and others) represent the pinnacle of current natural language processing technologies. These models have been pivotal in demonstrating capabilities close to human-level text understanding and generation, offering significant advantages in terms of scalability, cost-effectiveness, and ease of integration. The choice of GPT-X as our primary model is driven by its cost efficiency, which offers a low cost per token, and its stability\u2014being a product of a well-established company, it ensures reliability for long-term projects. Additionally, OpenAI provides robust APIs that facilitate straightforward web-based queries, an essential feature for seamless integration into diverse applications.</p> <p>In line with exploring other robust models to enhance our system's capabilities, we are also considering incorporating models like Microsoft's Bing, Meta's Llama, and the emerging Gemini model. Each of these models brings unique strengths:</p> <ul> <li>OpenAI GPT-X: Known for robust performance and versatility in various NLP tasks, making it ideal for core system operations.</li> <li>Bing: Utilizes Microsoft's extensive experience in AI and search technologies to provide enriched conversational capabilities.</li> <li>Llama: An OpenSource alternative, offers adaptability across different conversational contexts, ideal for applications requiring versatile linguistic styles.</li> <li>Gemini: Focuses on multitasking and handling multiple domains, which is crucial for systems requiring broad knowledge bases.</li> </ul> <p>Going further, the evolution from traditional LLMs to systems capable of AGI (Artificial Generalist Agents) involves creating models that can understand, learn, and perform any intellectual task comparable to human capabilities. AGI aims to transcend the limitations of typical NLP tasks by providing solutions that can apply learned knowledge across various disciplines. This shift is crucial for developing systems like GU-Systems, which require a level of cognitive flexibility and adaptability that goes beyond simple task execution.</p> <p>Autonomous AI takes this a step further by developing systems that operate independently of human oversight, which is essential for applications involving real-time decision making or when human input may introduce delays or biases. The ability to make autonomous decisions is critical for the areas of our project that involve real-time data interpretation and response generation without human intervention.</p>    AI State-Of-Art Diagram"},{"location":"LLMs-and-State-of-Art/#integrating-cognitive-agents-within-gu-systems","title":"Integrating Cognitive Agents within GU-Systems","text":"<p>To translate the sophisticated data processing capabilities of LLMs into actionable strategies in real-world applications, it is necessary to embed these models within Cognitive Agents. These agents utilize mechanisms that convert generated text and insights into direct actions, enabling effective operation in various environments. An excellent resource for current technologies and frameworks that facilitate the implementation of these agents is the awesome-AGI repository on GitHub, which showcases some of the most popular and cutting-edge repositories today.</p>"},{"location":"LLMs-and-State-of-Art/#framework-evaluation-for-optimal-integration","title":"Framework Evaluation for Optimal Integration","text":"<p>Our exploration of available frameworks through resources like the awesome-AGI repository revealed several potential candidates, including MetaGPT, AgentGPT, and AutoGPT:</p> <ul> <li> <p>MetaGPT: Targets professional and enterprise environments, facilitating rapid and secure AI integration within technological workspaces. However, its focus on enterprise solutions does not align with our project's need for broader applicability and flexibility.</p> </li> <li> <p>AgentGPT: Provides a user-friendly GUI and supports a collaborative AI-human working model. It is designed for semi-supervised operations, which do not meet the autonomy requirements of our project.</p> </li> </ul> <p>Ultimately, AutoGPT emerged as the optimal choice. It is selected for its active community support, stability, ease of integration, and its architecture that supports feedback-driven AI capable of utilizing skills/actions in a cognitive loop\u2014perfectly matching the requirements for GU-Systems. AutoGPT facilitates the integration of agents based on the Profile-Memory-Planning-Action model, which is extensively used across multiple disciplines for robust agent implementation. It includes features like:</p> <ul> <li>Forge System: Simplifies the deployment of new models, providing a platform where users can collaboratively develop and standardize AI technologies, which aligns with our vision of creating a flexible and adaptive system.</li> <li>Agent Protocol: This protocol standardizes interactions with AI agents via a REST API, streamlining the management of multiple agents and ensuring that GU-Systems can function efficiently as an Execution-Cognition Machine (ECM) that leverages these standards. Learn more about Agent Protocol</li> </ul>    AutoGPT/AgentProtocol BlackBox Diagram"},{"location":"LLMs-and-State-of-Art/#conclusions","title":"Conclusions","text":"<p>Understanding these state-of-the-art technologies and selecting the right frameworks like AutoGPT are crucial steps toward realizing the ambitious goals of GU-Systems. By integrating advanced AGI and autonomous AI capabilities, GU-Systems is set to redefine the landscape of interactive, cognitive AI applications, ensuring that our project remains at the cutting edge of technology innovation.</p>"},{"location":"MouseAgent-Insights/","title":"Introduction","text":"<p>The MouseAgent is a vision-based agent that uses LLM's capabilities to describe images in order to generate mouse intelligent movements and interactions. Thus, the main goal of this agent is to locate, move the mouse and interact with different elements in the user display.</p> <p>For obtaining this, multiple machine-learning based models can be approached, however using LLM's vision capabilities we empower graphical reasoning where the agent not only generates an action, but also thinks about the arrangement of the apps, buttons and elements in the display, obtaining a better adaptability to multiple scenarios than fine-tunning or using specialized models.</p>"},{"location":"MouseAgent-Insights/#design-insights","title":"Design Insights","text":"<p>MouseAgent uses an algorithm inspired on Mac Voice Control, where the user tells numbers to select where the mouse should move. We replicate this behavior by following these steps:</p> <ol> <li>Take a screenshot: This will be the first input to the LLM, where the agent must describe and analyse the general arrangement of the elements and reason about how to locate a specified element.</li> </ol> <p>A screenshot taken from an example environment.</p> <ol> <li>Draw a Grid: Split the screenshot into mutliple cells, each one with its correspondent number so the agent can describe a sector of the screen with just the generation of a label.</li> </ol> <p>Each sector of the screen is labeled into a cell.</p> <ol> <li> <p>Select Cell: The grid image is passed to the agent, which will reason about the possible cells containing the target element and will generate a number to zoom in.</p> </li> <li> <p>Zoom: Resize the selected cell and repeat the steps 3 and 4 until obtaining the desired accuracy.</p> </li> </ol> <p>An example of zoomed cell. Here the agent could find the button \"Code\"</p> <p>5 Inverse Resolution: Use inverse coordinate calculation in order to determine the pixel or coordinates of the center of the display's selected sector and use those coordinates in order to move the mouse to that location.</p>"},{"location":"MouseAgent-Insights/#implementation","title":"Implementation","text":"<p>All the grid drawing and computation has been designed into the <code>Grid</code> class at this file. It encapsulates the logic of zooming and selecting cells as shows:</p> <pre><code>\n    # Loading Screenshot\n    image_path = \"screenshot.png\"\n    image = Image.open(image_path)\n    grid = Grid.from_image(image, size=10)\n\n    # grid.image contains the grid drawing\n    plt.imshow(grid.image) \n    plt.show()\n\n    # You can make zoom into the cells\n    selection = int(input(\"Cell: \"))\n    sub_grid = grid.zoom(selection, size=5)\n    plt.imshow(sub_grid.image)\n    plt.show()\n\n   # You can also access the cells by using the grid attribute\n   print(grid.cells[9])\n</code></pre> <p>The class MouseDescriptor uses the Grid class to move the mouse. MouseAgent generates a graph that executes the algorithm explained in the previous section.</p> <p>```python</p>"},{"location":"MouseAgent-Insights/#the-mouseagent-can-be-called-with-just-one-command","title":"The MouseAgent can be called with just one command","text":"<p>MouseAgent.find(\"Amazon Icon at Firefox\") ``</p> <p>Note: higher iterations in the MouseAgent enables more accuracy, however, using more than 2 iterations has been shown to be ineffective for user displays.</p>"},{"location":"OCR-Engine/","title":"Introduction","text":"<p>The OCR Engine system in this project is modular, structured to provide a clean interface for Optical Character Recognition engines. It consists of three main components:</p> <ol> <li>The Engine Entry Point \u2014 the single import path for the current OCR model.</li> <li>The Template (Abstract Base Class) \u2014 defines the interface every OCR engine must follow.</li> <li>The Implementation \u2014 the currently used OCR engine, which complies with the template.</li> </ol>"},{"location":"OCR-Engine/#1-ocr-engine-entry-point","title":"1. OCR Engine Entry Point","text":"<p>The engine interface is centralized to allow flexibility and future upgrades without breaking compatibility. Whenever you need to use OCR in your code, import it from the unified path:</p> <pre><code>from cognition_layer.tools.ocr.engine import OCR\n</code></pre> <p>Do not import directly from the implementation (e.g., <code>rapidocr.engine</code>). This wrapper ensures you always interact with the most updated and validated OCR engine.</p>"},{"location":"OCR-Engine/#2-ocr-engine-template","title":"2. OCR Engine Template","text":"<p>All OCR engines must follow the structure defined by the <code>OcrEngine</code> abstract class. This ensures a consistent interface across implementations.</p>"},{"location":"OCR-Engine/#key-concepts","title":"Key Concepts","text":""},{"location":"OCR-Engine/#boundingbox","title":"BoundingBox","text":"<p>A <code>BoundingBox</code> is a data structure that represents a detected text area. It includes: - The four corner coordinates: <code>top_left</code>, <code>top_right</code>, <code>bottom_left</code>, <code>bottom_right</code> - The center point of the box - The recognized text (<code>content</code>) - Any optional <code>additional_info</code></p> <p>This structure allows precise localization of text in an image.</p> <pre><code>@dataclass\nclass BoundingBox:\n    top_left: int\n    top_right: int\n    bottom_left: int\n    bottom_right: int\n    center: tuple[int, int]\n    content: str\n    additional_info: dict = field(default_factory=dict)\n</code></pre>"},{"location":"OCR-Engine/#ocr-engine-class","title":"OCR Engine Class","text":"<p>All implementations inherit from <code>OcrEngine</code> and must implement the <code>invoke()</code> method:</p> <pre><code>class OcrEngine(ABC):\n    def invoke(self, image: Image.Image, *args, **kwargs) -&gt; list[BoundingBox]:\n        raise NotImplementedError()\n</code></pre> <p>Optionally, they can implement <code>clean()</code> to reset internal states.</p> <p>The engine also includes a <code>Storage</code> object to hold <code>latest_detections</code>, allowing multiple modules to access the last recognized content.</p>"},{"location":"OCR-Engine/#example-detecting-and-working-with-bounding-boxes","title":"Example: Detecting and Working with Bounding Boxes","text":"<p>Here is a complete example of how to use the OCR system to analyze an image:</p> <pre><code>from cognition_layer.tools.ocr.engine import OCR\nfrom PIL import Image\n\nocr = OCR()\nimage = Image.open(\"screenshot.png\")\n\nresults = ocr.invoke(image)\n\nfor box in results:\n    print(\"Text:\", box.content)\n    print(\"Center:\", box.center)\n</code></pre>"},{"location":"OCR-Engine/#what-you-can-do-with-bounding-boxes","title":"What You Can Do with Bounding Boxes","text":"<ul> <li>Highlight or annotate the detected text in a UI.</li> <li>Use <code>center</code> to guide cursor movement or simulate a click.</li> <li>Filter results by text content or region.</li> <li>Combine bounding boxes with other AI modules to understand context.</li> </ul> <p>This modular design ensures flexibility and extensibility while keeping the usage simple and consistent.</p>"},{"location":"Planex-Insights/","title":"Introduction","text":""},{"location":"Planex-Insights/#implementation-of-the-cognition-layer-algorithm-a_1-with-planex","title":"Implementation of the Cognition Layer Algorithm ($A_1$) with Planex","text":"<p>The Cognition Layer Algorithm, sometimes referred to as $A_1$, defines the intelligent or cognitive layer of the ECM. A detailed problem analysis can be found in the ECM Problem Analysis, where we divided the ECM cognition problem into three subproblems: Planify, Reduce, and Translate.</p>"},{"location":"Planex-Insights/#design","title":"Design","text":"<p>Planex has been designed as a three-agent model. By leveraging LangChain, we can utilize the capabilities of large language models (LLMs) such as GPT-3.5. In this context, we assume the knowledge base $C'$ aligns with the properties established in the analysis. This framework allows us to effectively define and chain these three agents, with their behavior primarily specified through two methods:</p> <ol> <li>Fine-Tuning: By fine-tuning the models, we can achieve the desired behavior from each agent more accurately. However, this methodology requires an unbiased dataset and extensive analysis/training.</li> <li>Prompt Engineering (Selected Approach): Using prompt engineering, we can quickly deploy the agents with an approximation of the fine-tuned behavior. Zero-shot prompts enable experimentation, modification, and testing of agent results.</li> </ol> <p>We selected the prompt engineering method because it allows for the rapid exploration of multiple agents and new approaches, in alignment with the easily replaceable modules of the ECM.</p>"},{"location":"Planex-Insights/#prompt-engineering","title":"Prompt Engineering","text":"<p>All the prompts designed can be found in the /cognition_layer/planex/agents/prompts.py file of the repository. In that file, each agent has three defined prompts:</p> <ol> <li>Instructions: Here we declare the expected behavior of the agent. As defined in Lei Wang et al. paper, we define the Profile of the agent. The main properties of each agent are:</li> <li>Planner: The planner's objective is \"to provide a detailed step-by-step plan to address the user's query.\"</li> <li>Reducer: The reducer's objective is \"to review a given plan and provide a new plan that achieves the same result using predefined functions.\"</li> <li> <p>Translator: The translator's objective is \"to translate the plan into Exelent language.\"</p> </li> <li> <p>Guidelines: Using a trial-and-error methodology, we define a set of rules that improve the agent's behavior by explaining and warning about possible failures and misconceptions about the goal. For example, \"Ensure the new plan uses only the Exelent language constructs and achieves the same result as the original plan.\"</p> </li> <li> <p>Example: By using one-shot prompting, we can improve the reliability and standardize the response of the model, specifying for each agent a possible result and its format.</p> </li> </ol> <p>For some agents we also provide some information about the system, such as which tools are available, which is the focused window, operative system, etc. For this step we could take advantage of langchain tool formatting for defining the valid tools the ECM can receive.</p>"},{"location":"Planex-Insights/#results","title":"Results","text":"<p>Using this simple but efficient model, we achieve the following advantages:</p> <ol> <li>Simple Task Solving: For tasks that require three or four consecutive steps, Planex can appropriately define and execute the correct actions. Examples include: \"Open Spotify\" and \"Write 'hello world' on the terminal.\"</li> <li>Fast and Controlled Results: The results are consistently defined after three steps, establishing a concrete number of steps to execute.</li> </ol> <p>The main disadvantages found while testing this agent are the following:</p> <ol> <li>Large Prompt: As this model uses a one-shot prompting methodology, it requires an extensive prompt each time it is executed, making the request more expensive and necessitating re-learning how to solve the query each time it is called. Fine-tuning could address this issue, but further research is needed.</li> <li>No Failure Reaction: If the reducer or translator fails to properly select the correct tools, the agent cannot recover and must be fully reloaded.</li> <li>Not Fully System Aware: Although we have introduced information about the system, the agent cannot fully understand and develop a mental simulation of the system's status. This limitation leads to failures where the agent assumes previously opened apps, defined requirements, etc.</li> </ol> <p>All this properties can be tested in this repository by using the following command:</p> <pre><code>python ecm/core/main.py --agent planex\n</code></pre> <p>If you are in a safe environment you can also use the python executable in /ecm/core/run_in_host.py</p>"},{"location":"Planex-Insights/#planexv2","title":"PlanexV2","text":"<p>PlanexV2, located in the directory /cognition_layer/planexv2, is a four-agent model for $A_1$ that improves upon the original Planex agent by introducing the Blamer, an agent capable of reacting to exceptions and recalling the failed agent.</p> <p>The Blamer follows the same schema as other agents, with three key properties:</p> <ol> <li>Exception Handling: The Blamer is only called when an exception occurs. It receives the exception as a string and provides the system with context information about all Planex agents.</li> <li>Response Specification: Using LangChain, we fully specify the format of the Blamer's response, defining three key concepts to resolve: the blamed agent (Who failed?), the explanation, and advice for avoiding the failure when the agent is called again.</li> <li>Selective Recall: The Blamer can recall agents from the Planex chain as needed, skipping those that are not necessary (i.e., those that are correct).</li> </ol>"},{"location":"Planex-Insights/#results_1","title":"Results","text":""},{"location":"Planex-Insights/#key-advantages","title":"Key Advantages","text":"<ol> <li>Failure Reaction: The agent can now recover from failures, reusing agents and ensuring that the response will return a valid Exelent file.</li> <li>Improved Accuracy: By showing the agents their failures, we achieve better results, with improved accuracy and more reliable plans.</li> </ol>"},{"location":"Planex-Insights/#key-disadvantages","title":"Key Disadvantages","text":"<ol> <li>Cost of Recovery: Recovering from a failure requires a larger prompt, making it more expensive than expected, even though some agents are skipped.</li> <li>Limited Recovery: The Blamer is not always able to fully recover Planex from failures. If the plan is too long (i.e., involves too many steps), PlanexV2 can enter a loop, failing to recover. To address this, PlanexV2 has a maximum step limit.</li> </ol> <p>All these properties can be tested in this repository using the following command:</p> <pre><code>python ecm/core/main.py --agent planexv2\n</code></pre>"},{"location":"Planex/","title":"Planex","text":"<p>[!WARNING] Planex is no longer maintained and is not included on the main file/bootstrap.</p>"},{"location":"Planex/#planex-as-an-approximation-to-a_1","title":"Planex as an Approximation to $A_1$","text":"<p>Planex is a sophisticated 3-step planner agent designed to approximate the ECM problem through the utilization of LangChain. This approximation involves the collaborative efforts of three distinct agents, each focusing on a crucial aspect of the process: Planning, Reduction, and Translation, as delineated in the Theoretical Analysis.</p> <p>To provide a comprehensive understanding, these three steps correspond to the following concepts: - Planning: The planning agent receives a natural language query from the user and devises a solution to approximate the problem. This process is independent of the specific context or tools available to the agents. - Reduction: The reduction agent takes the explanation of the problem provided by the planner and identifies a set of tools that correspond to each instruction. Consequently, the resulting plan includes the necessary keywords and tools for execution. - Translation: The translation agent receives the plan and generates an Exelent file, which can be executed to achieve the desired solution.</p>"},{"location":"Planex/#usage","title":"Usage","text":"<p>To begin, let us explore the implementation of these agents. All agents within Planex are located in the /agents directory. Each agent possesses an attribute <code>chain</code>, which contains the execution module of the chain. Additionally, each agent includes a function for direct execution with a query, as illustrated in the example below:</p> <pre><code>from cognition_layer.planex.agents.planner import Planner\nfrom cognition_layer.planex.agents.reducer import Reducer\nfrom cognition_layer.planex.agents.translator import Translator\n\nplanner = Planner()\nreducer = Reducer()\ntranslator = Translator()\n\n# Execute an example action for each agent\nplanner_result = planner.plan(\"Open Spotify\", verbose=True)\nreducer_result = reducer.reduce(planner_result.content, verbose=True)\ntranslator_result = translator.translate(translator.content, verbose=False)\nprint(\"Plan defined: \", translator_result.content)\n</code></pre> <p>It is important to note that each agent returns a BaseMessage from LangChain, ensuring both traceability and scalability. As a result, the output of the agents is contained in the .content attribute of the Messages.</p> <p>Upon executing the above code, you may observe that the plan references functions not yet defined. This occurs because the agents have not been provided with the specific actions they can utilize to solve the plan. To define new actions, you can register your functions within the ItemRegistry, and they will be automatically integrated into the planners.</p> <pre><code>from ecm.tools.registry import ItemRegistry\n\n@ItemRegistry.register_function\ndef click(place: str) -&gt; None:\n    \"\"\"Clicks on the specified place\"\"\"\n    print(\"Clicked on \", place)\n\n# If you instantiate the tool after initializing the agents, notify the reducer to update its actions:\nreducer.auto_bind_actions()\n</code></pre> <p>It is crucial that the docstring and typing are provided for the registered function, as this information is used by the agent to describe the function to the LLM.</p> <p>After registering the new actions, you can re-execute your three agents, and the defined actions will be considered when formulating the Exelent plan.</p>"},{"location":"Planex/#merging-chains","title":"Merging Chains","text":"<p>As mentioned earlier, each agent contains a LangChain chain. This allows for the combination of all agents into a single entity by pipelining the necessary messages for each agent:</p> <pre><code>planex = (\n    planner.chain\n    | (lambda output: {\"input\": output.content, \"actions\": reducer.actions})\n    | reducer.chain\n    | (lambda output: {\"input\": output.content})\n    | translator.chain\n)\n</code></pre> <p>You can now effortlessly execute Planex by invoking it as a chain:</p> <pre><code>result = planex.invoke({\"input\": query})\nprint(\"Result: \", result.content)\n</code></pre>"},{"location":"Planex/#planex-prompt-engineering","title":"Planex Prompt Engineering","text":"<p>The instructions defined for each agent in Planex can be found in the /agents/prompts.py module. This module contains multiple prompts that are included in each invocation. Each prompt comprises specific instructions detailing the target, guidelines to be followed, and examples to illustrate proper usage. You are encouraged to modify these prompts to explore potential new behaviors and functionalities of Planex.</p> <p>After defining the prompts in the prompts.py file, they are merged for each agent as follows:</p> <pre><code>sys_message = (\n    \"\\nInstructions: \\n\"\n    + PlanexPrompts.AGENT_INSTRUCTIONS\n    + \"\\nGuidelines: \\n\"\n    + PlanexPrompts.AGENT_GUIDELINES\n    + \"\\nExample:\\n\"\n    + PlanexPrompts.AGENT_EXAMPLE\n)\n\n# The agent here could be the planner, reducer, or translator \nagent.prompt = ChatPromptTemplate.from_messages(\n    [(\"system\", sys_message), (\"user\", \"{input}\")]\n)\n</code></pre> <p>In this implementation, the sys_message variable is constructed by concatenating the instructions, guidelines, and example prompts defined in the PlanexPrompts class. This combined system message is then assigned to the prompt attribute of the respective agent (planner, reducer, or translator). The ChatPromptTemplate.from_messages method is used to create a prompt template from these system and user messages, facilitating effective communication and task execution by the agents.</p> <p>Feel free to experiment with and refine these prompts to optimize the performance and adaptability of Planex to various scenarios and requirements.</p>"},{"location":"Pyxcel-Interpreter/","title":"Pyxcel Interpreter","text":""},{"location":"Pyxcel-Interpreter/#warnings-and-notes","title":"Warnings and Notes","text":"<ul> <li>Pyxcel is ROS Agnostic, fully created with python packages</li> <li>Pyxcel uses ItemRegistryV2, it is designed to be backward-compatible but the oldest agent tested is Xplore.</li> <li>Pyxcel ensures that the feedback messages \"RUNNING\" and \"FINISH\" are sent to callbacks. The callback function is equivalente to the ROSA feedback.</li> <li>Pyxcel can return results with the \"RESULT\" message on the feedback_callback.</li> </ul>"},{"location":"Pyxcel-Interpreter/#supported-properties","title":"Supported Properties","text":"Property Default Value Notes stop Yes hard_stop No (Use with caution) sync_calls Yes async_calls Yes Recommended wait_for No callbacks Yes Full control implemented with Feedback Messages feedback Yes callback_keywords <code>(\"RUNNING\", \"STEP\", \"SUCCESS\", \"ABORT\", \"FINISH\", \"SWITCH\")</code> RUNNING and FINISH are ensured for every task types <code>\"Sequential\"</code> Additional types may be implemented in the future <p>References: - Pyxcel Internals</p>"},{"location":"Pyxcel-Interpreter/#pyxcelinterpreter-attributes","title":"PyxcelInterpreter Attributes","text":"Attribute Type Description <code>callback_dict</code> <code>dict[str, ExecutionStatus]</code> Maps callback keywords to their execution statuses."},{"location":"Pyxcel-Interpreter/#examples","title":"Examples","text":"<p>Initializing and killing</p> <pre><code>from execution_layer.pyxcel.interprter.pyxcel_interpreter import PyxcelInterpreter\ninterpreter = PyxcelInterpreter()\ninterpreter.kill()\n</code></pre> <p>Running</p> <pre><code>import ecm.exelent.parser as parser\nmy_task = parser.parse(\"path_to_xlnt_file\")\n\ninterpreter.run(my_task) # Will return at finish\ninterpreter.arun(my_task) # Returns immediately\n</code></pre> <p>Stopping</p> <pre><code># Note: Stopping nested functions is not fully stable\ninterpreter.run(my_task)\ninterpreter.stop(my_task.name)\n</code></pre>"},{"location":"ROSA-Interpreter/","title":"ROSA Interpreter","text":""},{"location":"ROSA-Interpreter/#warnings-and-notes","title":"Warnings and Notes","text":"<ul> <li> <p>Rosa does support concurrent tasks, however only one Rosa can be instantiated at the time, this is due to ROS2 dependency. Don't worry about initing with <code>RosaInterpreter()</code> or <code>Rosa()</code> multiple times, it is programmed as a singleton, but don't try to instantiate various ROSA in multiple process.</p> </li> <li> <p>Human in the loop or waitings in ROSA can be easily handled with the tools you pass. Just communicate your functions by using Feedback Messages.</p> </li> <li> <p>ROSA only ensures \"RUNINNG\" and \"FINISH\" feedback messages will be sent to the callbacks. The other ones will depend on the code inside the tools.</p> </li> <li> <p>You can use the <code>gureg</code> command in the shell to debug in live and look what messages are being passed inside ROSA. Also you can use ROS2 topic tools or change the <code>LOG.LEVEL</code> inside the constants </p> </li> </ul>"},{"location":"ROSA-Interpreter/#supported-properties","title":"Supported Properties","text":"Property Default Value Notes stop Yes hard_stop Yes (Use with caution) sync_calls Yes async_calls Yes Recommended wait_for Yes callbacks Yes Full control implemented with Feedback Messages feedback Yes callback_keywords <code>(\"RUNNING\", \"STEP\", \"SUCCESS\", \"ABORT\", \"FINISH\", \"SWITCH\")</code> RUNNING and FINISH are ensured for every task types <code>\"Sequential\"</code>, <code>\"ControlledSequence\"</code> On develop the implementation of Parallel/Conditional tasks <p>References: - How to use Feedback Messages - Rosa Internals</p>"},{"location":"ROSA-Interpreter/#rosainterpreter-attributes","title":"RosaInterpreter Attributes","text":"Attribute Type Description <code>type_dict</code> <code>dict[str, SequenceType]</code> Maps sequence types to their implementations. <code>callback_dict</code> <code>dict[str, ExecutionStatus]</code> Maps callback keywords to their execution statuses."},{"location":"ROSA-Interpreter/#examples","title":"Examples","text":"<p>Initing and kill</p> <pre><code>from ecm.mediator.rosa_interpreter import RosaInterpreter\ninterpreter = RosaInterpreter()\ninterpreter.kill()\n</code></pre> <p>Running</p> <pre><code>import ecm.exelent.parser as parser\nmy_task = parser.parse(\"path_to_exln_file\")\n\ninterpreter.run(my_task) # Will return at finish\ninterpreter.arun(my_task) # Returns immediatly\n</code></pre> <p>Waiting:</p> <pre><code># Be careful with waiting, you must be sure the call will be raised or you could end blocked.\n# You can safely wait for FINISH, but for messages as STEP, SUCCESS or ABORT is better to use callbacks.\n\ninterpreter.arun(my_task)\ninterpreter.wait_for(my_task.name, \"FINISH\") # Equivalent to interpreter.run()\n</code></pre> <p>Stopping:</p> <pre><code>interpreter.arun(my_task)\ninterpreter.stop(my_task.name)\n</code></pre>"},{"location":"ROSA/","title":"Using ROSA (ROS Agent)","text":""},{"location":"ROSA/#introduction","title":"Introduction","text":"<p>ROSA (ROS Agent) is a crucial component in our system that simplifies the interaction with the Task Sequence Protocol (TSP) through a series of easy-to-use functions at higher levels. This guide covers the basics of how to utilize ROSA for handling tasks, executing sequences, and managing execution statuses within a test-driven development environment.</p>"},{"location":"ROSA/#setup-rosa","title":"Setup ROSA","text":"<p>Before executing any tasks, initialize the ROSA instance. Here's how to set up ROSA within your test environment:</p> <pre><code>from execution_layer.rosa.interfaces.rosa import ROSA\nrosa = ROSA()\n</code></pre>"},{"location":"ROSA/#executing-a-task","title":"Executing a Task","text":"<p>To execute a task, you need to define a SequencePackage with a list of ActionPackage items that specify what actions to perform. In the following example, we will use an example function that prints <code>Waiting...</code> 3 times and finishes printing \"Hello ROSA!\".</p> <p>For this, we will use a set of functions called <code>sleep_and_print</code> included in the mocks. If you want to use your own functions, you can look at the mocks and copy the templates with your own code. </p> <pre><code>from execution_layer.rosa.ros2.tools.packages import ActionPackage, SequencePackage, SequencePriority\n\nwaiting = ActionPackage(\n    action_id=ItemRegistry.get_id(sleep_and_print), text=\"Wating...\"\n)\nend = ActionPackage(\n    action_id=ItemRegistry.get_id(sleep_and_print), text=\"Hello ROSA!\"\n)\n</code></pre> <p>Now that we have a set of actions, we must define the behavior and priority of the actions. For this, we will use a SequencePackage. In this object, we will define the following properties:</p> <ul> <li><code>task_id</code>: Here you will define a name for the task. This will be used as an internal identifier of all the tasks with the same objective. Usually you should write the purpouse of your sequence, in this case <code>wait_and_print</code></li> <li><code>type</code>: Here you can select how to execute your set of actions. You can find all types in the types directory. However you can implement your own types, by loading them as stablished in the basic.py docs. Note that the most relevant types are the following:</li> <li><code>SimpleSequence</code>: Executes the sequence in order, if an action fails, it will return an ABORT code, else a SUCCESS code.</li> <li><code>ParallelSequence</code>: Yet not implemented.</li> <li><code>priority</code>: Stablishes the order in which the server will execute the tasks if multiple tasks with the same id are received. You can look at all priorities in this file</li> <li><code>actions</code>: An array with all the actions you want to perform</li> </ul> <pre><code>from execution_layer.rosa.ros2.types.basic import SimpleSequence\n\nseq = SequencePackage(\n    task_id=\"wait_and_print\",\n    type=SimpleSequence.get_type(),\n    priority=SequencePriority.NORMAL,\n    actions=[wait_for_text, end_text],\n)\n</code></pre> <p>Now that you have the task defined, we will create a task in ROSA and build a function for controlling the feedback returned from the server. For creating a feedback manager (optional) you can define a function for managing each Feedback object received from ROSA. A feedback object contains two main properties:</p> <ul> <li><code>_exec_status</code>: This value should not be modified, it returns some information of the status of the SequenceType. The most common values are the following:</li> <li><code>FINISH</code>: This value is always returned when the SequenceType finishes (wether it success or fails)</li> <li><code>ABORT</code>: Returned when the SequenceType fails</li> <li><code>SUCCESS</code>: Returned when the SequenceType has ended properly</li> <li><code>STEP</code>: Returned between the execution of each action</li> <li> <p><code>RUNNING</code>: Returned when the feedback has asynchronously been generated by an inside action/function</p> </li> <li> <p><code>object</code>: This value contains any object that you can return from inside the functions executed, use it as you want.</p> </li> </ul> <p>As an example we will make the following controller:</p> <pre><code>from execution_layer.rosa.ros2.tools.feedback import Feedback\nfrom execution_layer.rosa.ros2.tools.feedback import ExecutionStatus\n\ndef my_controller(feedback: Feedback):\n    if feedback._exec_status == ExecutionStatus.FINISH:\n        print(\"Task has finished!\")\n</code></pre> <p>Note that if you don't want to use any controller you can just use the <code>ROSA.muted_callback</code> as argument.</p> <p>Now that we have everything settled up, we just have to execute all with ROSA. Remember that ROSA will use a non-blocking call so if you want to wait for the execution to finish before continuing the program you can use the <code>wait_for</code> function.</p> <pre><code>self.rosa.new_task(\"wait_and_print\", feedback_callback=my_controller)\nself.rosa.execute(seq)\nself.rosa.wait_for(\"wait_and_print\", ExecutionStatus.FINISH)\n</code></pre>"},{"location":"ROSA/#stopping-a-task","title":"Stopping a task","text":"<p>Whenever you have launched a task, you may want to abort the following actions. For this reason ROSA implements 2 stopping methods. Soft-Stop and Hard-Stop.</p> <p>A soft-stop allows the current sequence to complete gracefully before stopping, it will raise an event into the SequenceType to stop executing whenever a clean exit is possible. This exit usually happens between action steps.</p> <pre><code>self.rosa.soft_stop(\"wait_and_print\")\nself.rosa.wait_for(\"test_soft_stop\", ExecutionStatus.FINISH)\n</code></pre> <p>A hard-stop immediately raises a signal to stop a task, disregarding its current state or any ongoing actions. This is a critical function for emergency situations where stopping a task as quickly as possible is necessary to prevent undesirable effects or to reset the system rapidly. However, this function should not be used unless it is stricly needed since it can end up in corrupted files or inconsistent executions.</p> <pre><code>import time\n\nself.rosa._hard_stop(\"wait_and_print\")\ntime.sleep(1)  # Wait to ensure the process has been terminated\n</code></pre>"},{"location":"ROSA/#using-feedback","title":"Using Feedback","text":"<p>Wherever you are in python, if you want to return feedback to the SequenceType that has called you (if any) remember that you can return an object with the following structure.</p> <pre><code>from execution_layer.rosa.ros2.tools.feedback import Feedback\n\nf = Feedback()  # Automatically will settle up your task_id\nf.publish(my_object)\n\n# Note: You should have built at least one ROSA instance before using feedback objects.\n</code></pre> <p>Now this feedback will be managed by a ROSA callback manager if it has been stablished in the task.</p>"},{"location":"ROSA/#conclusion","title":"Conclusion","text":"<p>This tutorial provides a foundational understanding of how to interact with ROSA to execute, monitor, and manage tasks within a ROS2-enabled environment. The methods showcased help facilitate effective testing and control of asynchronous task execution, essential for robust software development. Now you are ready to implement and improve your own functionalities with ROSA as an Execution_Layer Algorithm or A_2.</p>"},{"location":"RePlan-Insights/","title":"RePlan Insights","text":"<p>[!WARNING] RePlan is no longer maintained and is not included on the main file/bootstrap.</p>"},{"location":"RePlan-Insights/#introduction","title":"Introduction","text":""},{"location":"RePlan-Insights/#implementation-of-the-cognition-layer-algorithm-a_1-with-replan","title":"Implementation of the Cognition Layer Algorithm ($A_1$) with RePlan","text":"<p>RePlan is a ReAct-based agent that uses multiple tools to focus on control, permission, and failure reaction of the $A_1$ algorithm. To achieve this, we use the LangChain ReAct agent and chain each action done by the agent into an <code>agent_scratchpad</code> where the agent can think and review the actions and results obtained.</p> <p>The core of this agent resides in its tools. As LangChain states:</p> <p>Docs: \"Agents are only as good as the tools they have.\"</p> <p>The tools provided to RePlan are the following: 1. PlanexV2 as a Tool: We reused PlanexV2's capability to generate Exelent files from queries and reformatted it as a tool fully integrated into RePlan. This enables RePlan to planify from a given query. 2. ExecutionWrapper as a Tool: We integrated the control of the execution layer with a set of tools that enable the agent to listen to the feedback obtained from the actions, approve or deny actions, or create new tasks.</p>"},{"location":"RePlan-Insights/#results","title":"Results","text":""},{"location":"RePlan-Insights/#key-advantages","title":"Key Advantages","text":"<ol> <li>Reaction: The agent can react to failures, generating new plans or new approaches to the user query.</li> <li>Control: RePlan can accept or deny steps by taking advantage of the <code>ControlledSequence</code> type in Exelent.</li> <li>Better Results: Using RePlan results in better performance and more effective utilization of Planex actions.</li> </ol>"},{"location":"RePlan-Insights/#key-disadvantages","title":"Key Disadvantages","text":"<ol> <li>Token Cost: Minimizing the tokens used in RePlan is challenging, as the thinking process of ReAct trades reliability and usability for more expensive requests to the LLM.</li> <li>Not Exelent Aware: Although RePlan can generate Exelent code, it cannot modify the plan, relying on Planex for plan generation.</li> <li>Not Context Aware: ReAct can gather information about the system but encounters the same awareness issues as Planex when planning.</li> </ol> <p>This module can be tested like other agents by using the following command:</p> <pre><code># We recommend using --debug to fully trace all that happens inside RePlan \npython ecm/core/main.py --agent RePlan --debug\n</code></pre>"},{"location":"Remote-Execution/","title":"Remote Execution","text":"<p>[!WARNING] The latest version uses a client/server remote model based on the repository <code>/external/ecm_communications</code> please refer to that repository for updated docs.</p>"},{"location":"Remote-Execution/#ecm-serverclient","title":"ECM Server/Client","text":"<p>The ECM algorithms designed in this repository are aimed to be executed remotely, this is, to have a computer or machine that reasons, loads and computes all the cognitive layers ($A_1$) and a client machine that listens and executes all the functions sent by the server ($A_2$), this can be easily activated without affecting the core architecture by using the modules integrated in this repository.</p> <p>The Client/Server modules use RabbitMQ in order to stablish a connection between the two machines, to start this protocol you only need to start listening with the client as follows:</p> <pre><code>import action_space.experimental.mouse.actions # noqa\nimport action_space.experimental.screenshot.actions # noqa\nfrom ecm.remote.client import EcmClient\n\nif __name__ == \"__main__\":\n    client = EcmClient()\n    client.listen()\n</code></pre> <p>Note that we must import the actions we want to be enabled in the client machine in order to be auto-detected when received from the server. Warning: If the client is not connected, the server will fail when trying to send the correspondent action.</p>"},{"location":"Remote-Execution/#synchronizing-the-itemregistry","title":"Synchronizing the ItemRegistry","text":"<p>After having the client connected, we can use the ItemRegistry to send tasks to the client. For using the ItemRegistry appropriately you must differentiate between two types of capabilites:</p> <ul> <li>Registering Functions: This is the base function for sending actions to the client. When loading AI agents, the names of this functions and multiple descriptions on how to use them are loaded in order to let the AI decide when and why to send an action to the client. Registering an action can be done as follows:</li> </ul> <p><code>python   @ItemRegistry.register_function   def do_something(element: str):       \"\"\"Does something using the element. Usage do_something('foo')\"\"\"      ...</code></p> <ul> <li>Registering utils: Utils can be also sent to the client, though they won't be notified to the AI's, so you can use this tools to execute AI-hiden algorithms, such as taking screenshots for sending them to the AI, computing calculations, or obtaining information. This actions do not have to be AI-friendly.</li> </ul> <p><code>python   @ItemRegistry.register_util   def restart(arg1, arg2, arg3):      ...</code></p> <p>Finally, if you want to use your functions in the host machine, just don't apply changes, in the contrary, if you want to execute utils or functions in the remote machine, use the following command to automatically synchronize with the client:</p> <pre><code># The actions my AI will find\nfrom action_space.experimental.mouse.agent import MouseAgent # noqa\n\nfrom ecm.tools.registry import ItemRegistry\nif __name__ == \"__main__\":\n    ItemRegistry.transfer_execution_to_client()\n    ...\n    # execute your algorithms normally...\n</code></pre> <p>Warning: ItemRegistryV2 has deprecated this function. The replaced function is <code>EcmServer.wrap_item_registry()</code></p> <p>All the functions/utils will be sent to the client when trying to access them through the ItemRegistry. </p> <p>Warning: If you try to hard-access the functions without the ItemRegistry, they will be executed in the host machine. For this reason, multiple times you will see <code>ItemRegistry._utils[\"my_func\"](args...)</code> instead of calling the function directly.</p>"},{"location":"Remote-Execution/#modifying-connection","title":"Modifying connection","text":"<p>All the properties such as protocol, ip, etc, can be changed at the .env file. Do not share that file with anyone, it gives access to executing remote code to your machine.</p> <p>For configuring this file you must create a RabbitMQ user by using the following commands:</p> <pre><code># Add a new user\nsudo rabbitmqctl add_user &lt;USER&gt; &lt;PASSWORD&gt;\n\n# Giving access to the client\nsudo rabbitmqctl set_user_tags &lt;USER&gt; administrator\nsudo rabbitmqctl set_permissions -p / &lt;USER&gt; \".*\" \".*\" \".*\"\n</code></pre>"},{"location":"Rosa-Insights/","title":"Implementation of the Execution Layer Algorithm with ROSA ($A_2$)","text":"<p>The Execution Layer Algorithm, sometimes designated as $A_2$, plays a crucial role in the Execution-Cognition Machine (ECM) architecture. Its primary function is to transform a set of actions selected by the Cognition Algorithm ($A_1$) into executable actions that result in solving the user's problem. $A_2$ is responsible for the execution of these actions, ensuring that they are carried out in an orderly and effective manner, adapting to the needs of the problem and the execution context.</p>"},{"location":"Rosa-Insights/#implementation-of-a_2-using-sequenceactionserver-and-ros2-rosa-approximation","title":"Implementation of $A_2$ Using SequenceActionServer and ROS2 (ROSA Approximation)","text":"<p>For explaining the architecture of $A_2$ internals, we begin with the setup of our ECM at a low level. Since the theoretical expansion of A' should be supported by the execution layer, the following TSP protocol is proposed.</p>"},{"location":"Rosa-Insights/#231-task-sequence-protocol-tsp","title":"2.3.1 Task-Sequence Protocol (TSP)","text":"<p>Let's imagine the simplest case, suppose a problem ( p \u2208 P ) could be solved with a single action, such as pressing the letter \"K\" key in the keyboard. The Task-Sequence Protocol (PTS) encapsulates this action in an object called ActionPackage.</p> <p>An <code>ActionPackage</code> consists of two key elements:</p> <ul> <li> <p>Function: Reference to our action ( b \u2208 B ), such as a keystroke. Note how this function can be passed to the ActionPackage by using the ItemRegistry. Just ensure to register the function with the appropiate decorator and then pass the id to the package.</p> </li> <li> <p>Arguments: Here you can pass details relevant to the action, such as the specific key \"K\". These arguments can be passed as args or kwargs</p> </li> </ul> <p>After we are able to make actions. We want to be able to combine them in further ways, such as compositing, so we archive the A' expansion property. For this, using ROS2 Behavior Trees (BT) directly in our ECM would be impractical, as it would force the LLM to build an XML and a set of unnecessary components. Instead, we use an approach based on SequencePackages.</p> <p>A <code>SequencePackage</code> is a set of actions with behavior defined by the <code>SequenceType</code>, which determines whether the actions should be executed in parallel, linearly, conditionally, etc. This allows for the efficient execution of the keystroke action \"K\".</p> <p>When multiple sequences are related to achieving the same objective, they are encapsulated in what is called a <code>Task</code>. A task is a set of related sequences, ordered by priority (defined in each sequence individually). This allows the system to generate more complex plans that include concurrent action execution and even mechanisms for feedback, interruption, or sequence succession in a straightforward manner for the LLM. Although it is possible to send the task directly to the <code>TaskRegistry</code> controller, we will show a cleaner way to manage tasks with ROSA, so by now, just focus on on the how the priority of the task can affect the execution of the plan.</p>"},{"location":"Rosa-Insights/#sequenceactionserver-vs-rosa","title":"SequenceActionServer vs ROSA","text":"<p>The SequenceActionServer or SAS (also called IODA as a deprecated name) is responsible for converting requests from higher levels into executable actions for the Gateway using the TSP. It's crucial to remember the separation of responsibilities at higher and lower levels:</p> <ul> <li>ROSA: This layer is an interface for building request as a set of actions to be executed.</li> <li>SAS and Lower Levels: These are responsible for HOW to execute those actions, given a plan.</li> </ul> <p>Within the question of how to execute a plan given by a higher level, we find different problems to solve, differentiated into 3 modules represented in the following diagram:</p> <ul> <li> <p>ROS2 Topics: Using the communication functionalities offered by ROS2, we encapsulate all the Tasks from higher levels in JSON format, which can be read asynchronously through callbacks in the ActionClient (ROS2 Actions). In this way, all tasks will be published in the <code>/resquest</code> topic, while the feedback will be returned in <code>/feedback</code></p> </li> <li> <p>SequenceActionClient: Also known as \"Request Responder,\" it reads the requests published on the topics and applies the TSP protocol to define which behavior to execute next (interrupt, execute, put on hold, etc.). For each sequence it decides to start, it will receive feedback produced and forward it to higher levels for processing. Responsibility: Define WHEN to execute each sequence.</p> </li> <li> <p>SequenceActionServer: Also known as \"Request Controller,\" it receives sequences sent by the client and executes without the ability to cancel. Its responsibility is to define the type of execution according to the task's topology and its possible BT actions. Responsibility: Define WHO to call to execute the sequence.</p> </li> <li> <p>SequenceTypes: Encapsulated within the \"Request Controller\" in the general diagram, it is responsible for defining the order and conditions of execution for each received sequence. Responsibility: Define HOW to execute the received sequence.</p> </li> <li> <p>UPI Listener: Not shown in Figure 8, it is responsible for receiving asynchronous user requests sent from the UPI in the hardware-layer and sending them directly to higher layers via the ROS2 topics. Its response will be received in the same way as the rest of the sequences, with a different priority if necessary.</p> </li> </ul>"},{"location":"Rosa-Insights/#24-rosa-and-alb","title":"2.4 ROSA and ALB","text":"<p>In the previous sections, we have discussed the mechanism to implement a ECM at a low level. However, due to its complexity and level of abstraction, ROSA (ROS Agent) was created.</p> <p>ROSA serves as an interface that simplifies the entire TSP into a set of simple functions to be executed by higher layers. This means it converts the rest of the objects into ROS2-Agnostic, eliminating the restrictions caused by dependency among these components. ROSA effectively turns the entire system below it into a black-box model that EXECUTES the sent commands.</p> <p>On the other hand, (although is not necessary to interact with) to eliminate concurrency issues and the linking of functions from lower levels to higher levels, the ALB (Application Layer Builder) is responsible for launching all ROS2 nodes and maintaining the resources used within the same memory space of a single process (multithreaded). Thus, a set of objects created by different layers can be accessed without the need to use pipelines or alternative methods to the system. The ALB will be managed by ROSA so the user doesn't have to interact with ROS2 nodes.</p>"},{"location":"Self%E2%80%90Training-Agents/","title":"Introduction","text":"<p>In the paper Self-Trained Agents a research about training agents with an iterative mechanism is explored. As its fundamentals, the main process is to fine-tune an LLM by forcing it to reason about its world, correcting it and fine-tuning the agent with its own outputs. By using this process, we are able to fine-tune an agent without using a human generated.</p> <p>In this repository we will take use of this methodology in order to fine-tune the Cognition Layer ($A_1$) Agents in order to minimize the costs, delay and empower the reasoning capabilities of LLMs.</p>"},{"location":"Self%E2%80%90Training-Agents/#specialization-architecture","title":"Specialization Architecture","text":"<p>At the specialization.py module we can find a simple AI graph to implement Self-Training. This module can be used in order to generate samples (for a bigger dataset) that ensures the reasoning about a task is successful. The main algorithm uses the following steps:</p> <ol> <li> <p>Try: Receive the task from the user as a prompt, it then tries to generate an action that solves that query and predict the result that its action will generate in the user environment.</p> <p>Human: <code>\"Open the firefox browser\"</code></p> <p>AI: {<code>reasoning=\"The display...\"</code>, <code>action=\"open(\"Amazon\")</code>, <code>learning=None</code>, <code>expectation=\"The window will open\"</code>}</p> </li> <li> <p>Test: An <code>effect_descriptor</code> object is passed to the AI, this descriptor will be responsible of generating a description of the latests effects in the AI environment. With this description, the AI reasons if the expectation has been completed, and if so, the last reasoning will be returned for saving it as a sample.</p> </li> <li> <p>Learn: If the test has failed, then the AI tries to generate new learnings, reasoning about its failures and how it could overcome it in the next iteration. Reset the environment and return to the try node.</p> </li> <li> <p>Try: Iterate over the steps 1 to 3 adding the learnings obtained from previous nodes to the prompt of the agent.</p> </li> </ol> <p>An example usage of this module could be the following program:</p> <pre><code>if __name__ == \"__main__\":\n    # Build an executor (You could use for example RosaInterpreter...)\n    def test(query):\n      result = execute(query)\n      return result\n\n    # Build the graph\n    graph = SpecializationGraph(\n        action_prompt=\"You are an AI computer expert. You must provide...\",\n        test=test,\n        effect_descriptor=lambda: describe_environment(),\n        reset_state=lambda: reset_my_env(),\n    ).compile()\n\n    # Execute the graph and obtain a sample\n    user_input = \"I want to do foo\"\n    config = {\"configurable\": {\"thread_id\": str(random.randint(5000, 15000))}}\n    for event in graph.stream({\"query\": user_input}, config, stream_mode=\"values\"):\n        print(\"=\" * 30)\n        print(Fore.YELLOW + json.dumps(event, indent=4) + Fore.RESET)\n</code></pre>"},{"location":"Self%E2%80%90Training-Agents/#self-training-architecture","title":"Self-Training Architecture","text":"<p>At the specialization.py module you can find an alternative implementation of the previous architecture. In this case, we focus on improving the testing methods and making the architecture as an ubiquitous one. The main steps are the followng:</p> <ol> <li> <p>Try: Calls a function that will act emulating the \"try\" node of the Specialization graph, thus if the goal is to generate actions, the function will receive the user query, the learnings and previous failures in order to generate an action (function response)</p> </li> <li> <p>Execute: Calls a function that executes the action provided from the try node. In contrast with the specialization graph, the learn function must have a form to obtain the real solution of the query, if the solution is not correct, the learn node will be called. Also, if the maximum iterations of the agent have been reached, it will also fail, enabling to retry a new reasoning from the begging.</p> </li> <li> <p>Learn: Calls a function that contains generates a reasoning about what have failed and how it can be solved in posterior steps. The same architecture as in Specialization graph is used from now on.</p> </li> </ol> <p>An example usage of this module could be the following program:</p> <pre><code>if __name__ == \"__main__\":\n\n    # Build the graph\n    graph = SelfTrainGraph(\n        agent=lambda input: agent_reason(input),\n        response_executor=lambda agent_output: execute(agent_output),\n        test_approval=lambda execution_response: check(execution_response, expected),\n        learner=lamda graph_status: agent_generate_learnings(graph_status),\n        max_iterations=4, # If it fails more than 4 times exit.\n    ).compile()\n\n    # Run the graph\n    user_input = \"...\"\n    for event in graph.stream(\n        {\"query\": user_input, \"success\": False, \"current_iterations\": 0},\n        stream_mode=\"values\",\n    ):\n        result = event\n\n    # Return sample\n    return TrainingResponse(\n        iterations=result[\"current_iterations\"],\n        output=json.loads(result[\"response\"])[\"reasoning\"],\n        success=result[\"success\"],\n    )\n</code></pre>"},{"location":"SimpleCognitiveMemory/","title":"SimpleCognitiveMemory Documentation","text":"<p>The SimpleCognitiveMemory is a lightweight class designed to simplify the management of memory for AI systems, particularly when interacting with LLMs. It handles message storage, automatic pruning of old messages, preservation of important prompts, and optional removal of images to optimize token usage.</p>"},{"location":"SimpleCognitiveMemory/#key-features","title":"Key Features","text":"<ul> <li>Capacity Management: Automatically discards older messages when capacity is exceeded.</li> <li>Preservation of Crucial Messages: Allows preservation of initial messages, such as LLM instructions or initial prompts.</li> <li>Image Pruning: Optionally removes images from messages to reduce the number of tokens and lower memory usage.</li> </ul>"},{"location":"SimpleCognitiveMemory/#initializing","title":"Initializing","text":"<p>You can create an instance of <code>SimpleCognitiveMemory</code> by specifying: - <code>capacity</code>: The maximum number of messages to store (default is 5). - <code>keep_images</code>: Whether to keep image URLs in messages (default is <code>True</code>). - <code>preserve</code>: A list of messages to be preserved and never pruned (e.g., initial instructions).</p> <pre><code>from cognition_layer.memory.simple import SimpleCognitiveMemory\n\nmemory = SimpleCognitiveMemory(capacity=10, keep_images=False)\n</code></pre>"},{"location":"SimpleCognitiveMemory/#preserving-important-messages","title":"Preserving Important Messages","text":"<p>To ensure that critical messages like initial prompts or instructions are never discarded, use the <code>preserve</code> parameter:</p> <pre><code>from cognition_layer.memory.simple import SimpleCognitiveMemory\nfrom langchain_core.messages import SystemMessage\n\ninitial_instructions = [SystemMessage(content=\"Follow these rules strictly. [...]\")]\nmemory = SimpleCognitiveMemory(capacity=10, preserve=initial_instructions)\n</code></pre> <p>Note: The total number of preserved messages cannot exceed the specified capacity.</p>"},{"location":"SimpleCognitiveMemory/#updating-memory-with-new-messages","title":"Updating Memory with New Messages","text":"<p>To add new messages to the memory:</p> <pre><code>from langchain_core.messages import HumanMessage\n\nnew_messages = [HumanMessage(content=\"What is the weather today?\")]\nmemory.update(new_messages)\n</code></pre> <p>Older messages will be automatically removed when the capacity limit is reached.</p>"},{"location":"SimpleCognitiveMemory/#removing-images-automatically","title":"Removing Images Automatically","text":"<p>If you want to minimize the token cost by removing images from the memory buffer, set <code>keep_images</code> to <code>False</code> during initialization:</p> <pre><code>memory = SimpleCognitiveMemory(capacity=10, keep_images=False)\n</code></pre> <p>Alternatively, you can prune images from the messages manually using the utility function:</p> <pre><code>from cognition_layer.memory.simple import prune_images_from_messages\n\nprune_images_from_messages(memory.messages)\n</code></pre>"},{"location":"SimpleCognitiveMemory/#retrieving-messages","title":"Retrieving Messages","text":"<p>Access the current state of memory using the <code>messages</code> property. This will include both preserved and regular messages:</p> <pre><code>all_messages = memory.messages\n</code></pre>"},{"location":"SimpleCognitiveMemory/#example-usage","title":"Example Usage","text":"<pre><code>from cognition_layer.memory.simple import SimpleCognitiveMemory\nfrom langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n\n# Initialize memory with capacity and preserved instructions\ninstructions = [SystemMessage(content=\"You are a helpful assistant.\")]\nmemory = SimpleCognitiveMemory(capacity=5, preserve=instructions, keep_images=False)\n\n# Update memory with new messages\nmemory.update([AIMessage(content=\"Hello, how can I assist you today?\")])\n\nmemory.update([HumanMessage(content=\"What's the weather today?\")])\n\n# Retrieve all stored messages\nfor msg in memory.messages:\n    print(msg.content)\n</code></pre>"},{"location":"SimpleCognitiveMemory/#conclusion","title":"Conclusion","text":"<p>The <code>SimpleCognitiveMemory</code> class simplifies memory handling for LLMs by: - Automatically managing capacity and removing old messages. - Preserving crucial prompts or instructions. - Optionally reducing memory cost by removing images.</p> <p>This makes it ideal for efficient and scalable LLM-based applications where memory management and token optimization are critical.</p>"},{"location":"Storage/","title":"Introduction","text":"<p>The <code>Storage</code> class is a minimalistic alternative to the <code>ItemRegistry</code>, designed to act as a shared dictionary-like object that can synchronize its content across different parts of your program. Whether you're running code in separate threads or in different modules, you can use the same <code>Storage</code> instance simply by giving it the same name.</p> <p>Unlike <code>ItemRegistry</code>, it doesn't distinguish between \"tools\" or \"actions\". Instead, it's just a shared dictionary with instance-level persistence by name.</p>"},{"location":"Storage/#initializing-a-storage","title":"Initializing a Storage","text":"<p>You can create or retrieve a <code>Storage</code> instance by simply calling it with a name:</p> <pre><code>from ecm.tools.item_registry_v2 import Storage\n\ndefault_storage = Storage()\nmy_named_storage = Storage(\"my_shared_space\")\n</code></pre>"},{"location":"Storage/#sharing-the-same-instance","title":"Sharing the Same Instance","text":"<p>If you create two <code>Storage</code> objects with the same name, they will point to the same internal data, even if they are created in different files or threads.</p> <pre><code>s1 = Storage(\"shared\")\ns2 = Storage(\"shared\")\n\ns1[\"foo\"] = \"bar\"\nprint(s2[\"foo\"])  # -&gt; \"bar\"\n</code></pre> <p>By default, if no name is passed, <code>\"default\"</code> is used.</p>"},{"location":"Storage/#basic-usage","title":"Basic Usage","text":"<p>Since <code>Storage</code> behaves just like a dictionary, you can use all the common operations:</p> <pre><code>storage = Storage()\n\n# Set values\nstorage[\"username\"] = \"Sam\"\n\n# Get values\nprint(storage[\"username\"])  # -&gt; \"Sam\"\n</code></pre>"},{"location":"Storage/#when-to-use-it","title":"When to Use It","text":"<p><code>Storage</code> is especially useful when:</p> <ul> <li>You want to share state between different modules or systems without passing references explicitly.</li> <li>You need a temporary memory that all parts of your app can access.</li> <li>You are mocking a cache, configuration space, or dynamic variables that multiple agents or components should read/write to.</li> </ul>"},{"location":"Storage/#example-use-case-sharing-state-between-modules","title":"Example Use Case: Sharing State Between Modules","text":"<p>Suppose you're writing a pipeline with two components: a producer that collects data and a consumer that processes it.</p> <p>producer.py:</p> <pre><code>from ecm.tools.item_registry_v2 import Storage\n\ns = Storage(\"pipeline\")\ns[\"data\"] = [1, 2, 3]\n</code></pre> <p>consumer.py:</p> <pre><code>from ecm.tools.item_registry_v2 import Storage\n\ns = Storage(\"pipeline\")\nprint(s[\"data\"])  # -&gt; [1, 2, 3]\n</code></pre> <p>This works even if the producer and consumer are loaded at different times, as long as they use the same <code>\"pipeline\"</code> name.</p>"},{"location":"Storage/#summary","title":"Summary","text":"<ul> <li><code>Storage</code> is a global, name-based shared dictionary.</li> <li>It is initialized with a name and synchronized across the codebase.</li> <li>Behaves exactly like a regular dictionary.</li> <li>Ideal for simple shared memory use cases, mock caches, or cross-module communication.</li> </ul> <p>With <code>Storage</code>, you gain a simple but powerful way to keep parts of your system in sync without the overhead or complexity of larger registries.</p>"},{"location":"System-Architecture-Overview/","title":"System Architecture Overview","text":""},{"location":"System-Architecture-Overview/#system-architecture-overview","title":"System Architecture Overview","text":"<p>The architecture for an Execution-Cognition Machine (ECM) is modeled after understanding the necessary theory to implement it. This documentation aims to detail the system's general architecture by revisiting the formal definition of an ECM. The ECM implementation requires two algorithms that meet specific criteria to effectively translate a user's problem into executable actions using the knowledge integrated into a Large Language Model (LLM).</p> <p>Thus, as the formal conclusion of the theoretical approximation stated, we are seaching for two algorithms that satisfy:</p> <p>Algorithm_1(p | C') = \u03bb | \u03bb \u2282 A' \u2227 Algorithm_2(\u03bb) = s</p> <p>In this way, the ECM architecture contains:</p> <ol> <li> <p>Cognition Algorithm (A1): This algorithm translates a problem ( p ) using the knowledge from an LLM ( C' ) into a subset of actions ( A' ) within a defined action set ( A' ). It reflects the ECM's \"cognition\" aspect.</p> </li> <li> <p>Execution Algorithm (A2): This algorithm takes the subset of actions ( A' ) and the problem ( p ) to produce a solution ( s' ). Since ( C' ) does not directly influence ( s' ), this algorithm can be simplified to ( A_2(p | A') = s' ). This reflects the \"execution\" aspect of the ECM.</p> </li> </ol>"},{"location":"System-Architecture-Overview/#architecture-components","title":"Architecture Components","text":"<p>The ECM is designed to interconnect three key modules: \"Cognition\", \"Execution\", and \"Action Space\". The following diagram illustrates the proposed architecture and the interconnections between these modules:</p>"},{"location":"System-Architecture-Overview/#detailed-architecture-explanation","title":"Detailed Architecture Explanation","text":"<ul> <li> <p>MCA (Main Core Agent): This class is responsible for maintaining an event loop connecting all the main modules in the ECM. It is built with the rest of the modules with the CELB.</p> </li> <li> <p>Action Space (B'): This is where all possible actions that the system can perform are defined. It serves as the foundational layer from which the Cognition Algorithm selects a subset of actions ( A' ).</p> </li> <li> <p>Cognition Space(A_1): This module processes the user-defined problem ( p ) using the integrated knowledge ( C' ) from the LLM (taking advantage of AutoGPT capabilities) to determine the appropriate actions ( A' ) from the Action Space. The output is a refined set of actions tailored to solve the problem expressed in an Exelent File. Note that all the system should be agnostic to the changes into the cognitive agent thanks to the AgentProtocol (acting as a mediator).</p> </li> <li> <p>Execution Space(A_2): Once the subset of actions ( A' ) is identified, this module takes over to execute these actions to achieve the desired outcome ( s' ). It operates independently of the integrated knowledge ( C' ), focusing solely on action implementation. As the moment of writing this, the responsible class for doing this is ROSA, however the implementation should be easily changed by using the mediator.</p> </li> <li> <p>Interpreter: As a middleware betwwwn A_1 and A_2, the interpreter is the class responsible for translating the specifications written in the Exelent file from the Cognitive Module into a set of objects/commands. If the Exelent file contains a set of plugins, this should be handled too by the interpreter, returning some actions to be resolved to the MCA.</p> </li> </ul>"},{"location":"System-Architecture-Overview/#ecmv2-architecture-expansion","title":"ECMv2 Architecture Expansion","text":"<p>In the realm of artificial intelligence, the trend towards multi-agent systems reflects a move towards more distributed and specialized processing. Expert Agents are specialized entities designed to handle specific tasks or domains, such as gaming or data analysis, with greater efficiency and expertise than a general agent could. This specialization allows for a more scalable system that can adapt and respond more dynamically to complex environments or specific user requirements.</p> <p>For this reason, the version V2 of the ECM aims to expand the action space ( A' ) without the need of more pre-composition or programming of each action. This concept stems from the current trend towards multi-agent architectures, where the main agent delegates the cognitive burden to specialized agents referred to as Expert Agents.</p> <p>In this context, the Exelent language will now need to incorporate new statements that define a set of new requests from the cognition space. These requests will be forwarded by the Main Core Agent (MCA) to the appropriate Expert Agents. For instance, if the task involves playing a video game, instead of burdening the cognition space with this task, it could redirect the request to an agent specialized in gaming.</p> <p>Notably, through the use of the AgentProtocol, we can employ the same interface as a mediator to maintain system stability and prevent modifications that could negatively impact the performance or functionality of the architecture. This ensures that any integration of new agents or capabilities remains plug and play, adhering to the established system architecture without disruptive changes.</p> <p>With this, ECMv2's design philosophy enhances the system's adaptability and scalability by leveraging specialized expertise and maintaining a modular, plug-and-play approach through standardized protocols. This ensures that as the demands of tasks evolve, the architecture can easily integrate new capabilities without losing efficiency or operability.</p>"},{"location":"Theoretical-Fundamentals/","title":"Theoretical Fundamentals","text":""},{"location":"Theoretical-Fundamentals/#problem-analysis-and-theoretical-approach","title":"Problem Analysis and Theoretical Approach","text":"<p>With the objectives established at the Introduction chapter, our focus initially is on addressing each of these objectives individually. This involves connecting LLMs (Large Language Models) to a standard user, defined as someone lacking programming skills and familiarity with non-standard software tools beyond graphical applications like office tools. This suggests developing a \"black box\" approach regarding both the user and the LLM itself.</p>"},{"location":"Theoretical-Fundamentals/#the-agis-problem","title":"The AGI's Problem","text":"<p>For obtaining the objectives stablished, it is important to dive into the concept of AGI's. AGI's (Artificial General Intelligence) can understand, learn, and apply knowledge across a broad range of tasks, similarly to human intelligence. An AGI can theoretically perform any intellectual task that a human can do, applying its intelligence generally across different domains without specific training for each.</p> <p>Implementing an AGI is practically impossible; however, we will approach the problem by axiomatizing that a human can solve any software problem p \u2208 P using a knowledge source C, and a set of actions A, such that:</p> <p>HumanSolution(p | C, A) = s</p> <p>Where s is the solution to problem p and is also a heuristically viable solution in a Turing machine environment.</p>"},{"location":"Theoretical-Fundamentals/#definition-of-ecms-as-an-approximation-to-the-agi-problem","title":"Definition of ECM's as an approximation to the AGI problem","text":"<p>We can approximate the problem by considering the knowledge of an LLM as C' \u2248 C, i.e., an estimator of human knowledge; and the set of programmable actions as A' \u2248 A, i.e., an estimator of the action space to solve p. Therefore, the creation of an AGI implies designing an algorithm that verifies the following equality:</p> <p>Algorithm_1(p | C', A') = s</p> <p>Finally, since C' is already integrated knowledge in LLMs by definition and fully accessible, we only need to design an architecture that acts as an intermediary between the knowledge C' and our problem p to achieve a set of actions A'. Execution of A' should reach our solution s, verifying the following equality:</p> <p>Algorithm_1(p | C') = \u03bb | \u03bb \u2282 A' \u2227 Algorithm_2(\u03bb) = s</p> <p>From now on, we will refer the Algorithm_1 or A_1 as the Cognitive Layer of the problem, where the objective is given a set of actions A', obtain the set of actions \u03bb \u2208 A' that solve the problem; and we will refer the Algorithm_2 or A_2 as the Execution_Layer of the problem, where the objective is given a set of actions \u03bb, execute them in order to reach the solution s.</p>"},{"location":"Theoretical-Fundamentals/#objectives-simplified","title":"Objectives Simplified","text":"<p>The problem of \"how to implement an AGI\" is now simplified into three subproblems: 1. Send the user's problem request p into a query q that the LLM can understand q'. 2. Convert the LLM's natural language response into a set of executable actions \u03bb \u2208 A'. 3. Execute the set of actions \u03bb.</p> <p>This approach is underpinned by the philosophy: \"If a human can solve it, then an LLM should also be able to\". With this idea in mind, we will refer to the implementation of an architecture that accomplish the properties stablished as an Execution-Cognitive Machine or ECM</p>"},{"location":"Theoretical-Fundamentals/#autogpt-in-relation-with-agis-problem","title":"AutoGPT in relation with AGI's Problem","text":"<p>AutoGPT is a Python library that uses AI Agents based on the Memory-Profile-Planning-Action (PMPA) model discussed in this paper. It allows programmers to implement agents connected via HTTP requests to GPT-OpenAI and similar models (e.g., LLAMA, even with local execution) easily and maintained by thousands of users collaboratively.</p> <p>A significant feature of this library is that it allows these agents to be equipped with a set of \"skills\" known as \"actions\" or \"abilities,\" with which the agent will know how to interact and thus call sections of code that the programmer must have previously implemented in the agent.</p> <p>The most powerful agent model implementing this technology is @evo.ninja, which successfully stands out for its abilities to solve software tasks/problems as shown in Figure 2 (use of files, I/O, CSV reading, and query resolution...).</p> <p>Despite the achievements by AutoGPT, this library is far from fully integrating our ECM, as its action space A' is a considerably small subset given the set of problems p \u2208 P, i.e., A' \u2248 A. Therefore, the goal of GU-S will be to extend the \"skills\" architecture of AutoGPT to one in line with a ETM.</p>"},{"location":"Theoretical-Fundamentals/#a-expansion-method","title":"A' Expansion Method","text":"<p>Given the AutoGPT AGI's problem, we define the A' Expansion as an alternative to better approach this problem.</p> <p>Before implementing the algorithm itself, we must establish the following axiom:</p> <p>Manually programming a set of actions A' for the entire subset P is not viable due to scalability issues.</p> <p>In simple terms, \"it is not feasible to program all possible actions and solutions that AutoGPT should use.\" Thus, GU-Systems proposes simplifying this problem to implementing a protocol with which to describe an Action Space B' robust enough so that the composition of actions in B generates a space A' \u2248 A.</p> <p>In robotics planning problems, this issue has previously been raised, where a widespread solution even today is the use of known as BehaviorTrees. These systems use a set of simple algorithms that can be combined to generate a virtually scalable action tree for any problem p. That is, BehaviorTrees provide us an easy solution to \"How to build complex plans with simple actions.\" Now we just need to determine WHICH will be these \"simple actions\" and determine if they are robust enough to reach a solution s'.</p> <p>Thus, our actions b \u2208 B are a set of commands and keystrokes (clicks, action on keyboard, etc.), and by combining these actions through a behavior tree, we can expand the action space B such that BT(B) \u2248 A' \u2248 A With this in mind, we have just made a significant approximation to the problem of ECMs, where we also obtain the following properties:</p> <ol> <li>The more generalist the set B, the greater the convergence of A' with respect to A</li> <li>The set of actions B is scalable at the programmer level</li> <li>The resolution of the problem p \u2208 P can be solved, regardless of the API or program with which it is interacting</li> </ol>"},{"location":"Usage-Guide/","title":"Usage Guide","text":"<p>This guide explains how to run the ECM and make use of the Cognition Layer, Execution Layer, and Action Space.</p>"},{"location":"Usage-Guide/#how-to-run","title":"How to Run","text":"<p>To start the ECM, run:</p> <pre><code>python ecm/core/main/main.py\n</code></pre> <p>[!NOTE] If you installed ECM using Conda, activate the environment first:</p> <p><code>pwsh conda activate ecm</code></p> <p>By default, ECM will launch with:</p> <ul> <li>Cognition Layer: <code>latest</code></li> <li>Execution Layer: <code>pyxcel</code></li> <li>Safe Mode: enabled (only displays actions without executing them)</li> </ul>"},{"location":"Usage-Guide/#requirements-before-launch","title":"Requirements Before Launch","text":"<p>Make sure you have the necessary API keys set up. These can be:</p> <ul> <li>Stored as environment variables, or</li> <li>Placed in a <code>.env</code> file in the root of the repository (default when installed)</li> </ul>"},{"location":"Usage-Guide/#command-line-arguments","title":"Command Line Arguments","text":"Flag Description <code>--verbose</code> Enable verbose agent output <code>--debug</code> Enable full debug logging for ECM <code>--langchain-debug</code> Enable debug output for LangChain <code>--agent</code> Choose which Cognition Layer to use (<code>darkvfr</code>, <code>fastreact</code>, etc.) <code>--executor</code> Choose Execution Layer (<code>pyxcel</code> or <code>rosa</code>; default: <code>pyxcel</code>)"},{"location":"Usage-Guide/#safe-mode-default","title":"Safe Mode (default)","text":"<p>By default, actions will not be executed. They will only be printed.</p>"},{"location":"Usage-Guide/#full-experience-mode","title":"Full Experience Mode","text":"<p>To allow ECM to fully control the machine:</p> <pre><code>python ecm/core/main/main.py --host\n</code></pre> <p>[!IMPORTANT] Use this mode only in a safe environment, as ECM will execute real actions (e.g., open apps, click with the mouse etc.).</p>"},{"location":"Usage-Guide/#distributed-setup","title":"Distributed Setup","text":"<p>ECM supports distributed operation using server and client roles:</p> <ul> <li><code>--server</code>: This machine acts as the main ECM controller (Cognition + Execution Layer)</li> <li><code>--client</code>: This machine receives tasks and executes them (Controlled)</li> </ul> <p>If no flags are passed, ECM will run both layers locally.</p> <p>You can also enable peer autodiscovery:</p> <ul> <li><code>--autodiscover</code>: Allows pairing client and server. Use it the first time with both roles.</li> <li><code>--localhost</code>: Allows autodiscover to use local machine as client/server</li> </ul>"},{"location":"Usage-Guide/#using-the-ecm","title":"Using the ECM","text":"<p>Once the system is initialized, you\u2019ll be prompted:</p> <pre><code>Request a Task:\n</code></pre> <p>You can enter natural language instructions. Examples:</p> <ul> <li><code>Open Spotify and play a song</code></li> <li><code>Search a tech offer on Amazon using Edge</code></li> </ul> <p>The Cognition Layer will process the query and generate an executable plan, which is sent to the Execution Layer.</p>"},{"location":"Usage-Guide/#logs-and-troubleshooting","title":"Logs and Troubleshooting","text":"<ul> <li> <p>In server/client mode:</p> </li> <li> <p>Server logs include reasoning and agent output</p> </li> <li> <p>Client logs only show action-related feedback</p> </li> <li> <p>In local mode, all logs appear in the same terminal</p> </li> <li>The agent handles most errors internally</li> </ul> <p>To STOP ECM:</p> <pre><code>Ctrl+C\n</code></pre>"},{"location":"Usage-Guide/#example","title":"Example","text":"<p>https://github.com/user-attachments/assets/83f4a0a1-e1f0-4623-bb2a-9c21101ba42b</p>"},{"location":"VFR-and-Variations/","title":"VFR and Variations","text":"<p>Note: To fully understand this entry, please first refer to the FastReact documentation which explains the core principles behind the ReAct (Reason + Act) methodology that VisualFastReact extends.</p> <p>In this article, we will introduce several variants derived from FastReact (FR), each being a better aproximation of previous ones.</p>"},{"location":"VFR-and-Variations/#visualfastreact-vfr","title":"VisualFastReact (VFR)","text":"<p>VisualFastReact is an advanced cognitive agent that enhances the original FastReact by introducing a vision-focused workflow. It integrates visual perception capabilities, allowing the agent to process and interact with screen content more efficiently. VisualFastReact is particularly optimized for environments where vision-based reasoning and action are necessary, such as user interfaces or image-rich tasks.</p> <p>The core idea is to split the reasoning process into two phases:</p> <ol> <li>Visual Analysis: The agent decides where to focus on the screen (full or partial region).</li> <li>Action Execution: Once focus is established, the agent repeatedly executes FastReact steps until it determines that a focus change is required.</li> </ol> <p>This hybrid approach significantly reduces token consumption by avoiding the repeated analysis of entire images.</p>"},{"location":"VFR-and-Variations/#key-benefits","title":"Key Benefits","text":"<ul> <li>Token Efficiency: By focusing only on relevant screen areas, the agent reduces image processing costs.</li> <li>Hybrid Visual Reasoning: Combines OCR, icon recognition, and localized image analysis.</li> <li>FastReact Integration: Maintains the speed and structure of FastReact's reasoning/action cycle.</li> <li>Cognitive Memory: Retains contextual information and summaries to support efficient, consistent decision-making.</li> </ul>"},{"location":"VFR-and-Variations/#visualfastreact-workflow","title":"VisualFastReact Workflow","text":""},{"location":"VFR-and-Variations/#1-visual-analyzer-node","title":"1. Visual Analyzer Node","text":"<ul> <li>Captures a screenshot (full screen).</li> <li>Compresses the image for token optimization.</li> <li>Uses an LLM to:</li> <li>Describe relevant screen information.</li> <li>Reason about the next step.</li> <li>Determine screen focus (e.g., \"top left\", \"fullscreen\").</li> <li>Return a natural language summary of the step.</li> </ul>"},{"location":"VFR-and-Variations/#2-fastreact-node","title":"2. FastReact Node","text":"<ul> <li>Crops the image based on the current focus.</li> <li>Analyzes icons and OCR-detected text.</li> <li>Builds a prompt with available tools and visual descriptions.</li> <li>Uses FastReactPrompt to reason and propose the next Python action.</li> <li>Returns:</li> <li>Reasoning.</li> <li>Python function to execute.</li> <li>The next focus state: <code>maintain_focus</code>, <code>change_focus</code>, <code>replanning</code>, or <code>finish</code>.</li> </ul>"},{"location":"VFR-and-Variations/#conclusion","title":"Conclusion","text":"<p>VisualFastReact builds upon FastReact to create a visually intelligent, token-efficient agent. By isolating visual analysis and leveraging a structured reasoning/action loop, it offers a scalable approach to screen interaction tasks. The system is modular, memory-aware, and interpretable, making it suitable for a wide range of cognitive tasks involving vision.</p>"},{"location":"VFR-and-Variations/#minimalvfr","title":"MinimalVFR","text":"<p>MinimalVFR is a simplified and modular variant of VisualFastReact (VFR), designed to maintain the core vision-and-reasoning loop of FastReact while increasing flexibility and reducing complexity.</p>"},{"location":"VFR-and-Variations/#overview","title":"Overview","text":"<p>MinimalVFR operates in a continuous state loop, allowing the LLM to reason and act directly, including changing its visual focus as a meta-action. This is a major departure from the original VisualFastReact design, which was structured around discrete states (<code>visual_node</code>, <code>fast_react_node</code>).</p>"},{"location":"VFR-and-Variations/#key-changes-from-visualfastreact","title":"Key Changes from VisualFastReact","text":"<ul> <li>Single-State Execution Loop: Instead of jumping between vision and action states, the LLM can now use meta-actions (see ActionSpace documentation) to change its focus on-screen, streamlining the execution cycle.</li> <li>Modular Function Breakdown: The agent's logic is split into clear, overridable methods. This enables the creation of new agents\u2014VFR variants\u2014by simply overriding or extending key methods, without rewriting the entire agent.</li> <li>Structured Output Only: MinimalVFR only uses structured outputs for reasoning + action, making it more robust against hallucinations and easier to validate.</li> <li>Graph-Free Architecture: No graph frameworks (like LangGraph) are used, simplifying the architecture.</li> </ul>"},{"location":"VFR-and-Variations/#execution-flow","title":"Execution Flow","text":""},{"location":"VFR-and-Variations/#1-initialization","title":"1. Initialization","text":"<ul> <li>Initializes cognition state, tool formatting, and memory.</li> <li>Uses <code>SimpleCognitiveMemory</code> to store messages.</li> </ul>"},{"location":"VFR-and-Variations/#2-screenshot-and-prompt-generation","title":"2. Screenshot and Prompt Generation","text":"<ul> <li>Captures and crops screenshots based on the current screen focus.</li> <li>Reduces image quality to save tokens.</li> <li>Builds prompt using:</li> <li>Instructions</li> <li>Formatted tools</li> <li>Memory history</li> <li>Current cognition state summary</li> </ul>"},{"location":"VFR-and-Variations/#3-fastreact","title":"3. FastReAct","text":"<ul> <li>The LLM receives the structured prompt and returns:</li> <li>A reasoning message</li> <li>An action to execute</li> <li>If a function is returned, it's compiled to an Exelent task and executed.</li> <li>The result (if any) is captured in the memory.</li> </ul>"},{"location":"VFR-and-Variations/#meta-actions","title":"Meta-Actions","text":"<p>Meta-actions enable the LLM to change screen focus mid-task without transitioning between system states. This increases responsiveness and allows the agent to adapt focus based on context naturally. Refer to the ActionSpace meta-actions documentation for more details.</p>"},{"location":"VFR-and-Variations/#why-use-minimalvfr","title":"Why Use MinimalVFR?","text":"<ul> <li>Encourages reusability and rapid prototyping of new VFR variants.</li> <li>Better alignment with LLM-based reasoning through structured outputs and minimal external dependencies.</li> </ul>"},{"location":"VFR-and-Variations/#summary","title":"Summary","text":"<p>MinimalVFR is a foundational agent for building flexible, vision-enabled cognitive systems. Its modular structure and continuous reasoning loop offer a powerful alternative to state-based agents like VisualFastReact. By simplifying both the architecture and development workflow, it paves the way for robust and customizable VFR variants.</p>"},{"location":"VFR-and-Variations/#darkvfr","title":"DarkVFR","text":"<p>DarkVFR is a ultra-lightweight, fast and vision-efficient VFR variant designed to reduce LLM token usage by shifting most visual reasoning outside of the LLM. It operates mostly \"blind,\" relying on structured reasoning and delegating visual interpretation to specialized, cheaper vision tools.</p> <p>The <code>Dark</code> preffix in the name does reference to the mainly interaction with the environment being mostly blind.</p> <p>As its main core, instead of feeding a screenshot to the LLM on every step, the agent works in a \"dark mode\" where screenshots are only shared when the LLM explicitly requests them through a meta-action (<code>look_to_screen</code>). When needed, these screenshots are low-resolution and used primarily for context.</p> <p>Additionally, the agent's action space is extended with new vision tools that call cheaper external models (e.g., MolMo or Moondream) to extract information from the image.</p>"},{"location":"VFR-and-Variations/#key-features","title":"Key Features","text":""},{"location":"VFR-and-Variations/#1-screenshot-on-demand","title":"1. Screenshot on Demand","text":"<ul> <li>Screenshots are not included by default.</li> <li>The LLM must explicitly request a screenshot via a meta-action (<code>look_to_screen</code>).</li> </ul>"},{"location":"VFR-and-Variations/#2-ultra-low-quality-image-processing","title":"2. Ultra-Low-Quality Image Processing","text":"<ul> <li>When screenshots are provided, they are significantly compressed:</li> <li><code>fullscreen</code>: 10% quality</li> <li><code>partial</code>: 20% quality</li> <li>These images are enough for coarse context and delegated to vision tools when necessary.</li> </ul>"},{"location":"VFR-and-Variations/#3-external-visual-tools","title":"3. External Visual Tools","text":"<ul> <li>The action space includes tools connected to external, specialized vision models.</li> <li>These models provide:</li> <li>OCR</li> <li>Icon detection</li> <li>Contextual image summaries</li> <li>LLM only reasons on processed results, not raw pixel data.</li> </ul>"},{"location":"VFR-and-Variations/#benefits","title":"Benefits","text":"<ul> <li>Massive Cost Reduction: From $0.30\u2013$0.50 per task to ~$0.005.</li> <li>Reasoning-Centric: Vision is used only as needed; most reasoning is symbolic and language-based.</li> <li>Flexible &amp; Scalable: Supports high-throughput tasks on resource-constrained systems.</li> <li>Reuses MinimalVFR: All functionality from MinimalVFR is inherited and only two methods are overridden.</li> </ul>"},{"location":"VFR-and-Variations/#summary_1","title":"Summary","text":"<p>DarkVFR is a cost-optimized, reasoning-first cognitive agent that extends the VFR family. It empowers the LLM to decide when vision is necessary, uses minimal image data, and delegates image understanding to cheaper and specialized models. It is ideal for large-scale or budget-sensitive applications where task accuracy must be balanced with efficiency.</p>"},{"location":"What-is-GU/","title":"What is GU","text":""},{"location":"What-is-GU/#what-is-gu-systems","title":"\ud83c\udf10 What is GU-Systems? \ud83c\udf1f","text":""},{"location":"What-is-GU/#introduction-to-deep-learning-models","title":"Introduction to Deep Learning Models","text":"<p>In the vast landscape of Artificial Intelligence, terms like Llama, GPT-4, and Gemini are more than just buzzwords. They represent state-of-the-art Deep Learning models, specifically Large Language Models (LLMs) trained with data from across the globe and powered by millions of parameters. These models are pioneering achievements that bring us closer to the utopia of intelligent machines with capabilities such as \"common sense\" and complex action planning that respond in seconds to queries processed on cloud-based servers.</p> <p>Despite having models that unlock groundbreaking technological properties, we are encumbered by significant hardware and software limitations: - Most devices lack the computational power needed to run these models locally. - There is no standard methodology for implementing \"intelligent algorithms\" across APIs, applications, or software tools, requiring AI engineers to intervene and adapt applications for enhanced behaviors.</p> <p>Current solutions like the AIs integrated into Microsoft Word, Canva, or Bing are impressive but just scratch the surface of what's possible. </p> <p>Our project\u2014codenamed GU-Systems (a nod to \"GatUniverse-Systems\" or \"Cat Universe Systems\")\u2014aims to revolutionize how we interact with and standardize AI technologies. GU-Systems is engineered to: - Democratize access: Make intelligent tools accessible to the average user, removing the need to \"search the internet for a specific tool\" for problem-solving. - Simplify AI integration: Help new generations of AI programmers by standardizing how intelligent algorithms are implemented across different projects.</p>"},{"location":"What-is-GU/#gu-systems-objectives","title":"GU-Systems Objectives","text":"<p>GU-Systems is focused on delivering a dual promise: 1. User Accessibility: Enabling average users to leverage the capabilities of LLMs in Autonomous Computer Interaction through an intuitive interface. 2. Software Community Empowerment: Facilitating the software community to standardize and implement new AI technologies effortlessly.</p>"},{"location":"What-is-GU/#how-we-bridge-the-gap","title":"How We Bridge the Gap","text":"<p>GU-Systems is not just a device but a movement to standardize and simplify the integration and scaling of AI technologies across various platforms. By providing a unified interface, we are not only enhancing user experience but are also paving the way for a future where AI can be more seamlessly integrated into daily applications.</p>"},{"location":"Xplore-Insights/","title":"Introduction","text":""},{"location":"Xplore-Insights/#implementation-of-the-cognition-layer-algorithm-a_1-with-xplore","title":"Implementation of the Cognition Layer Algorithm ($A_1$) with Xplore","text":"<p>Xplore is a Graph-based agent that cycles into a loop focusing on describing, planning subgoals and reviewing or reacting to the current status by using the $A_2$ algorithm. It is a minimalist improvement of RePlan enhancing the reasoning capabilities by enforcing reasoning over the images obtained from the display.</p>"},{"location":"Xplore-Insights/#design-insights","title":"Design Insights","text":"<p>Using LangGraph we can design a graph of agents that interacts with each other in order to obtain complex behaviors. The Xplore core graph is the following: </p> <ul> <li> <p>General Planner: This agent is responsible for generating a general plan to satisfy a query. This plan will be generated as a set of strings (plan) with a description of the status of the system and a reasoning on how the plan could be solved. This reasoning contains 3 mandatory labels to complete:</p> </li> <li> <p><code>description</code>: This ensures the agent reason about the current status of the system, identifying relevant properties such as which windows are open, the user OS, etc.</p> </li> <li><code>reasoning</code>: This label makes the agent reason about how the goal can be reached.</li> <li> <p><code>plan</code>: This label is where the plan (set of subgoals as strings) are shortly described following the previous reasoning.</p> </li> <li> <p>Subgoal Planner: This agent is responsible for generating an action or set of actions (with the names according to the actions registered in the action space) in order to satisfy a subgoal. This agent contains 2 mandatory labels:</p> </li> <li><code>reasoning</code>: A reasoning about what the user has requested and how that subgoal could be reached.</li> <li> <p><code>steps</code>: The set of actions to execute in order to complete the goal.   Note we force the agent to reason again about the subgoal, since this subgoal could be not fully described, so this step reinforces the precission and accuracy of the action generated.</p> </li> <li> <p>Interpreter: This node is responsible of translating the set of actions generated by previous nodes into Exelent code that will be sent to $A_2$. When the interpreter has executed all the actions, it will return.</p> </li> <li> <p>Review Completed: This node is responsible for calling an agent that will receive an image of the previous status of the machine and a posterior status after executing all the actions generated. Then it will remove from the subgoal list those subgoals that have been already reached. If all subgoals have been completed, it ends the execution, else it sends all the subgoals to the subgoal planner.</p> </li> </ul>"},{"location":"Xplore-Insights/#vision-capabilities","title":"Vision Capabilities","text":"<p>In contrast with Planex or RePlan, Xplore have vision capabilities to integrate images from the display into the reasoning. For reaching this, each node that executes in Xplore, is provided with a screenshot of the current status of the display. LLM's such as OpenAI GPT-4o model enable the possibility of passing this images as zero-shot prompts, enhancing the reasoning of the agent fully adapting to the multiple variables of different systems.</p> <p>An example screenshot passed to Xplore</p>"},{"location":"Xplore-Insights/#results","title":"Results","text":""},{"location":"Xplore-Insights/#key-advantages","title":"Key Advantages","text":"<ul> <li>Adaptability: By using visual capabilities, xplore is able to adapt to multiple environments, taking care of the arrangement of the windows, buttons, etc.</li> <li>Target Aiming: Xplore uses both long-term and short-term planning for building the following actions. This awareness boosts the planning process by simplifying uncertain future status and focusing closer actions.</li> <li>Explainability: By reasoning after each step we can define why the agent selects each action and what is its pourpuse.</li> <li>Cause-Effect Aware: By reviewing the effects of each action the agent takes, makes the agent able to react to incorrect steps and correct the plan to a viable solution.</li> </ul>"},{"location":"Xplore-Insights/#key-disadvantages","title":"Key Disadvantages","text":"<ul> <li>Velocity: By reasoning and using visual capabilities in each action the velocity of each action has been reduced to 50s/task (aprox.).</li> <li>Backward Planning: The Agent can't recover from certain actions where the next subgoal cannot be completed without replanning all the task or adding correction subgoals.</li> <li>Hallucinations and Icon difficulties: Not all elements from the display are correctly interpreted from the agent, where visual icons, buttons or drawings are misunderstood.</li> </ul> <p>In the next video, we provide an example execution of Xplore. Video</p>"},{"location":"_Footer/","title":"Footer","text":"<p>Navigation: Home | Repository</p> <p>All content is licensed under a Reserved License unless otherwise noted.</p>"},{"location":"_Sidebar/","title":"Sidebar","text":"<ul> <li>Home</li> <li>Introduction</li> </ul> <p>\ud83d\udd27 Quickstart - Installation Guide - Usage Guide</p> <p>\u2728 State of Art - LLM's and Frameworks - AgentProtocol - Bibliography</p> <p>\ud83e\uddea Architecture   - Theoretical Fundamentals   - Architecture Overview   - Architecture Guide   - ECM Problem Analysis</p> <p>\ud83d\udee0\ufe0f Tutorials   - ItemRegistry   - Exelent   - Interpreters   - FastAgentProtocol   - Cognitive Memory   - Storage   - OCR Engine</p> <p>\ud83d\udd79\ufe0f Action Space:   - Action Space   - mouse/ostools   - mouse/ocr   - mouse/model-based   - meta/fake   - meta/cognition-state   - keyboard/ostools   - vision/moondream   - packages/apps\u2010management   - packages/sleep   - experimental/screenshot</p> <p>\ud83d\udd0d Cognition Layer   - VFR and Variations   - FastReact   - Xplore   - MouseAgent   - Self Training Agents</p> <p>\ud83d\udca1 Execution Layer   - RosaInterpreter   - PyxcelInterpreter</p> <p>\ud83d\udce6 Miscellaneous   - Bare Metal Installation</p> <p>\ud83d\udd1a Deprecations   - ROSA Tutorial   - ROSA TaskSequence Protocol   - Planex Tutorial   - Planex Insights   - RePlan   - Cognition Layer API   - Remote Execution</p>"},{"location":"experimental-screenshot/","title":"Experimental screenshot","text":"experimental/screenshot <p>The <code>experimental/screenshot</code> module provides a tool, not an action, that captures a screenshot of the current screen and returns it as a base64-encoded string. This tool is designed to be used by the environment or host program, not directly by the agent.</p> Tool <code>experimental/screenshot.screenshot()</code> <ul> <li> <p>Purpose: Captures the full screen and returns it as a base64-encoded PNG string.</p> </li> <li> <p>Returns: A <code>str</code> representing the image encoded in base64.</p> </li> </ul> Important Notes <ul> <li> <p>This is a tool, not an agent action. It must be invoked from the host program or environment managing the agent.</p> </li> <li> <p>The returned image is not directly usable by the agent, since it cannot handle image data natively. The developer must manage the image.</p> </li> <li> <p>The use of a base64 string allows easy transmission over network (e.g., in a client-server architecture).</p> </li> </ul> How to Use the Image <p>To use the result of the tool, decode the base64 string using the provided utility function:</p> <code>load_image(image_bytes: str)</code> <ul> <li> <p>Purpose: Decodes the base64 string into a <code>PIL.Image</code> object.</p> </li> <li> <p>Returns: <code>PIL.Image</code></p> </li> </ul> <p>Example:</p> <pre><code>from action_space.tools.image import load_image\n\nimage_string = experimental/screenshot.screenshot()\nimage = load_image(image_string)\nimage.show()\n</code></pre> Summary  Tool | Description -- | -- screenshot() | Captures the screen and returns a base64 PNG string load_image(image_bytes) | Decodes the base64 image into a PIL.Image object   <p>Use this module to integrate real-time screen captures into agent workflows where the host environment is responsible for image visualization or processing.</p>"},{"location":"keyboard-ostools/","title":"keyboard/ostools","text":"<p>The <code>keyboard/ostools</code> action family provides the agent with low-level capabilities to use keyboard input. It supports writing full strings and pressing key combinations, such as hotkeys or shortcuts. This is essential for automating UI interactions that require keyboard use.</p> <p>These actions work across platforms:</p> <ul> <li>On Windows, they rely on <code>pyautogui</code>.</li> <li>On Linux, they use <code>uinput</code> to simulate key events at the system level.</li> </ul>"},{"location":"keyboard-ostools/#actions","title":"Actions","text":""},{"location":"keyboard-ostools/#keyboardostoolswritetext","title":"<code>keyboard/ostools.write(text)</code>","text":"<p>Simulates typing a string character by character, including support for special characters (e.g., <code>!</code>, <code>@</code>, etc.).</p> <ul> <li> <p>Parameter: <code>text</code> - A string to type.</p> </li> <li> <p>Example:</p> </li> </ul> <p><code>python   keyboard/ostools.write(\"Hello, World!\\n\")</code></p> <p>This types <code>Hello, World!</code> and then presses ENTER.</p>"},{"location":"keyboard-ostools/#keyboardostoolspress_keyskeys","title":"<code>keyboard/ostools.press_keys(keys)</code>","text":"<p>Simulates pressing multiple keys together as a combination (hotkey).</p> <ul> <li> <p>Parameter: <code>keys</code> - A list of key names. Must specify the side if applicable (e.g., <code>LEFTCTRL</code>, <code>RIGHTALT</code>).</p> </li> <li> <p>Example:</p> </li> </ul> <p><code>python   keyboard/ostools.press_keys([\"LEFTCTRL\", \"F5\"])</code></p> <p>This triggers the common \"refresh page\" shortcut.</p> <ul> <li>Example:</li> </ul> <p><code>python   keyboard/ostools.press_keys([\"WIN\", \"D\"])</code></p> <p>Minimizes all windows and shows the desktop.</p> <ul> <li> <p>Notes:</p> </li> <li> <p>Ensure correct naming of modifier keys (e.g., <code>LEFTSHIFT</code>, <code>RIGHTCTRL</code>).</p> </li> <li>On Windows, keys are converted to a compatible format automatically.</li> </ul>"},{"location":"keyboard-ostools/#platform-behavior","title":"Platform Behavior","text":"<ul> <li>Windows: Uses <code>pyautogui.write()</code> and <code>pyautogui.hotkey()</code>.</li> <li>Linux: Uses <code>uinput</code> and a character-to-keycode mapping, simulating real hardware input.</li> </ul> <p>This enables reliable keyboard simulation even under restricted environments (e.g., sandboxed or Wayland-based systems).</p>"},{"location":"keyboard-ostools/#summary","title":"Summary","text":"<ul> <li><code>keyboard/ostools.write(text)</code> types text character by character.</li> <li><code>keyboard/ostools.press_keys(keys)</code> sends key combinations.</li> <li>Platform-agnostic interface for agents to simulate typing and shortcuts.</li> </ul> <p>Use these actions for precise, low-level keyboard control across different systems.</p>"},{"location":"meta-cognition%E2%80%90state/","title":"meta/cognition_state","text":"<p>The <code>meta/cognition_state</code> action family enables agents to modify their internal state\u2014a structured object called the <code>CognitionState</code>. This functionality supports meta-actions, allowing agents to reason about and alter their own cognitive configuration.</p>"},{"location":"meta-cognition%E2%80%90state/#purpose","title":"Purpose","text":"<p>Instead of acting on the environment, these actions allow the agent to:</p> <ul> <li>Switch reasoning modes (e.g., <code>reasoning_mode = \"plan-first\"</code>)</li> <li>Signal internal flags (e.g., <code>has_finished = True</code>)</li> <li>Store intermediate results</li> </ul> <p>These changes do not directly affect the world but influence the agent's future behavior.</p>"},{"location":"meta-cognition%E2%80%90state/#how-it-works","title":"How it works","text":""},{"location":"meta-cognition%E2%80%90state/#1-initialization-external","title":"1. Initialization (external)","text":"<p>Before the agent starts, the host program must initialize the <code>CognitionState</code> with a defined schema (using Pydantic).</p> <pre><code>from action_space.meta.cognition_state.state import CognitionState\nfrom pydantic import BaseModel\n\nclass AgentState(BaseModel):\n    has_finished: bool = False\n    reasoning_mode: str = \"default\"\n\nCognitionState(schema=AgentState)\n</code></pre>"},{"location":"meta-cognition%E2%80%90state/#2-agent-receives-the-summary","title":"2. Agent receives the summary","text":"<p>Each reasoning iteration provides the agent with a summary of the current CognitionState:</p> <pre><code>{\n  \"has_finished\": {\n    \"type\": \"bool\",\n    \"description\": \"\",\n    \"value\": \"False\"\n  },\n  \"reasoning_mode\": {\n    \"type\": \"str\",\n    \"description\": \"\",\n    \"value\": \"default\"\n  }\n}\n</code></pre>"},{"location":"meta-cognition%E2%80%90state/#3-agent-updates-the-state","title":"3. Agent updates the state","text":"<p>If the agent wants to change one of these values, it calls the action:</p> <pre><code>meta/cognition_state.set_value_on_cognition_state(key=\"has_finished\", value=\"True\")\n</code></pre> <p>This action is intercepted by the runtime system, which updates the CognitionState accordingly.</p>"},{"location":"meta-cognition%E2%80%90state/#4-on-next-iteration","title":"4. On next iteration","text":"<p>The new state will be visible in the next reasoning cycle through the updated summary. This allows the agent to:</p> <ul> <li>Track what has been completed</li> <li>Switch strategies</li> <li>Coordinate multi-step plans</li> </ul>"},{"location":"meta-cognition%E2%80%90state/#example-use-case","title":"Example Use Case","text":"<p>An agent completing a planning task may signal that it has finished:</p> <pre><code>meta/cognition_state.set_value_on_cognition_state(key=\"has_finished\", value=\"True\")\n</code></pre> <p>This sets an internal flag. On the next cycle, the agent sees:</p> <pre><code>\"has_finished\": {\n  \"type\": \"bool\",\n  \"value\": \"True\"\n}\n</code></pre> <p>This can be used to exit loops, reduce verbosity, or change prompt behavior.</p> <p>[!NOTE] The cognition state attributes can be accessed by using the method <code>cognition_state.get(key)</code>.</p> <p>[!NOTE] All cognition states are saved in the Storage with the key <code>Storage(\"CognitionState\")[\"default\"]</code>. A key can also be passed in the initialization of the cognition state and will be stored as <code>Storage(\"CognitionState\")[\"your_key\"]</code></p>"},{"location":"meta-cognition%E2%80%90state/#summary","title":"Summary","text":"<ul> <li><code>meta/cognition_state</code> actions modify internal agent state, not the environment.</li> <li>State is defined via a schema and shared across reasoning steps.</li> <li>The cognition layer manages state updates and summary generation.</li> </ul> <p>This mechanism enables structured, trackable self-modification for LLM agents.</p>"},{"location":"meta-fake/","title":"meta/fake","text":"<p>The <code>meta/fake</code> action family contains placeholder or no-op actions that do not directly perform any effect within the agent's current reasoning cycle. Instead, they signal intent to the system executing the agent, allowing for richer interactions beyond the standard Action Space limitations.</p> <p>These actions are useful when the agent needs to influence its future inputs or behavior, particularly when dealing with data types not supported directly by actions (e.g., images).</p>"},{"location":"meta-fake/#example-metafakelook_to_screen","title":"Example: <code>meta/fake.look_to_screen</code>","text":"<p>The action:</p> <pre><code>meta/fake.look_to_screen()\n</code></pre> <p>signals that the agent wants to receive an image of the user's screen in the next iteration.</p> <p>Since actions can only return primitive types (strings, numbers, etc.), it is the responsibility of the runtime environment (the external program managing the agent) to:</p> <ul> <li>Detect that the agent requested a <code>meta/fake</code> action.</li> <li>Interpret the intention.</li> <li>Execute the side effect (e.g., capture a screenshot).</li> <li>Provide the resulting image back to the agent as input.</li> </ul>"},{"location":"meta-fake/#purpose","title":"Purpose","text":"<ul> <li>Enables stateful control and richer agent-environment loops.</li> <li>Allows agents to plan steps that go beyond text-based outputs.</li> <li>Promotes a unified interface where all modifications, even meta ones, happen through standard actions.</li> </ul>"},{"location":"meta-fake/#summary","title":"Summary","text":"<ul> <li><code>meta/fake</code> actions are not executed by the agent itself.</li> <li>They must be interpreted and handled externally.</li> <li>They provide a clean way to let the agent request data or future state changes.</li> </ul> <p>Use them when the agent needs to affect the environment or request non-textual input in a structured, trackable way.</p>"},{"location":"mouse-model%E2%80%90based/","title":"Mouse model\u2010based","text":"mouse/model-based <p>This entry describes the <code>mouse/model-based</code> actions, which allow an agent to click on an element on screen using only a natural language description\u2014without requiring any OCR or text detection step.</p> <p>There are two implementations available:</p> <ul> <li> <p><code>mouse/moondream-based</code></p> </li> <li> <p><code>mouse/molmo-based</code></p> </li> </ul> <p>Both follow the same procedure but differ in the model used to interpret the description and locate the element on screen.</p> How it works <ol> <li> <p>The agent calls the action with a textual description of the UI element to click. Example:</p> <pre><code>mouse/moondream-based.click('Edge on the windows bar')\n</code></pre> </li> <li> <p>The action sends the full screen image and the description to the visual grounding model (Molmo or Moondream).</p> </li> <li> <p>The model returns one or more predicted coordinates where the described element appears.</p> <ul> <li> <p>If multiple points are returned, the first one is clicked and a warning is issued.</p> </li> <li> <p>If no points are found, an error is returned.</p> </li> </ul> </li> <li> <p>A click is performed using the <code>mouse/ostools</code> action at the selected point.</p> </li> </ol> <code>location</code> argument <p>To improve precision and speed, both actions accept a required <code>location</code> argument that restricts the area where the model should look for the element. Valid options include:</p> <ul> <li> <p><code>top</code></p> </li> <li> <p><code>bottom</code></p> </li> <li> <p><code>left</code></p> </li> <li> <p><code>right</code></p> </li> <li> <p><code>center</code></p> </li> <li> <p><code>fullscreen</code></p> </li> </ul> <p>Example:</p> <pre><code>mouse/molmo-based.click('settings icon', location='bottom')\n</code></pre> Comparison: Molmo vs Moondream  Feature | Moondream | Molmo -- | -- | -- Speed | ~1 second | ~8 seconds Accuracy | Low | High Cost | Very low | High Usage Recommendation | Fast, low-stakes decisions | Critical UI interaction   <p>In summary:</p> <ul> <li> <p>Use Moondream for fast, lightweight interactions where high precision is not critical.</p> </li> <li> <p>Use Molmo when precision matters, even at the cost of speed and compute.</p> </li> </ul> <p>These actions are ideal for agents that need to interact visually with the screen when textual cues (like OCR) are unavailable or insufficient.</p>"},{"location":"mouse%E2%80%90ocr/","title":"action_space/mouse/ocr_based vs action_space/mouse/labelled-ocr","text":"<p>This document explains the differences and usage of two mouse control actions based on visual recognition: <code>mouse/ocr-based</code> and <code>mouse/labelled-ocr</code>. These actions allow a cognitive agent to interact with the graphical interface by recognizing screen elements via OCR (Optical Character Recognition) and label annotation.</p>"},{"location":"mouse%E2%80%90ocr/#mouseocr-based","title":"mouse/ocr-based","text":"<p>The <code>mouse/ocr-based</code> action relies solely on text recognition through an OCR engine. It works in two phases:</p>"},{"location":"mouse%E2%80%90ocr/#phase-1-ocr-detection-pre-requisite","title":"Phase 1: OCR Detection (Pre-requisite)","text":"<p>Before invoking the <code>mouse/ocr-based</code> action, the agent must perform OCR detection using:</p> <pre><code>from cognition_layer.tools.ocr.engine import OCR\n</code></pre> <p>Calling the OCR tool stores detection results in <code>ocr.latest_detections</code>. These include all recognized text bounding boxes on the screen. The <code>mouse/ocr-based</code> action requires this to be done beforehand. If not, the action will fail due to the lack of reference data.</p>"},{"location":"mouse%E2%80%90ocr/#phase-2-text-matching-and-click","title":"Phase 2: Text Matching and Click","text":"<p>Once text detections are available, the action searches for the text that best matches a given <code>target</code>. For example, if the target is:</p> <pre><code>mouse/ocr-based.click(\"Git\")\n</code></pre> <p>And the OCR engine has detected the text \"GitHub\", it will match it and perform a click on the center of the corresponding bounding box.</p>"},{"location":"mouse%E2%80%90ocr/#example-workflow","title":"Example Workflow","text":"<ol> <li>The agent is initialized.</li> <li>The agent runs OCR on the screen.</li> <li>The OCR engine returns detected texts like: <code>YouTube</code>, <code>GitHub</code>, <code>Settings</code>.</li> <li>The agent decides to click on <code>GitHub</code>, so it executes:</li> </ol> <pre><code>mouse/ocr-based.click(\"Git\")\n</code></pre> <ol> <li>The action finds the best matching box and clicks it.</li> </ol>"},{"location":"mouse%E2%80%90ocr/#mouselabelled-ocr","title":"mouse/labelled-ocr","text":"<p>The <code>mouse/labelled-ocr</code> action provides greater flexibility by allowing the agent to target not just visible texts, but also graphical elements like icons. It introduces a labelling step that augments each OCR bounding box with a semantic tag.</p> <p>The labeller used can be found at <code>/cognition_layer/tools/ocr/labeller</code></p>"},{"location":"mouse%E2%80%90ocr/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li>Run OCR as in the <code>ocr-based</code> approach.</li> <li>Labelling: Each OCR bounding box is passed through a labeller which assigns a label (e.g., <code>A2</code>, <code>F2</code>, <code>BH8</code>).</li> <li>Agent Selection: The agent sees these labels and selects one (e.g., <code>A2</code>).</li> <li>Freeze Labeller: Once a label is chosen, the current state of the labeller is frozen for global access.</li> <li>Click Execution: The agent calls:</li> </ol> <pre><code>mouse/labelled-ocr.click(\"A2\")\n</code></pre> <p>This will identify the bounding box associated with the label and perform a click on its center.</p>"},{"location":"mouse%E2%80%90ocr/#visual-representation-options","title":"Visual Representation Options","text":"<p>The labelled bounding boxes can be shown to the agent in two different ways:</p>"},{"location":"mouse%E2%80%90ocr/#1-labels-overlaid-on-screenshot","title":"1. Labels Overlaid on Screenshot","text":"<p>This image can be obtained by using the <code>labeler.draw()</code> method</p>"},{"location":"mouse%E2%80%90ocr/#2-labels-displayed-in-a-table","title":"2. Labels Displayed in a Table","text":""},{"location":"mouse%E2%80%90ocr/#this-image-can-be-obtained-by-using-the-labelerboard-method","title":"&gt; This image can be obtained by using the <code>labeler.board()</code> method","text":"<p>Both actions are compatible with Windows and Linux, and they provide powerful tools for screen-based interaction using vision and semantics.</p>"},{"location":"mouse%E2%80%90ostools/","title":"Mouse\u2010ostools","text":"ostools - Move mouse to absolute coordinates <p>This entry explains how the ostools action works within the Action Space of the ECM. This basic action allows a cognitive agent to move the mouse to a given pair of screen coordinates <code>(x, y)</code> and perform a click.</p> <p>These actions can be added to the ItemRegistry with the path:</p> <pre><code>import action_space.mouse.ostools.actions\n</code></pre> Summary Table  Platform | Positioning Method | Click Method | Notes -- | -- | -- | -- Windows | pyautogui | pyautogui | Works reliably Linux (X11) | pyautogui | pyautogui | Works reliably Linux (Wayland) | uinput + calibrated movement | uinput | Requires setup and calibration, bypasses Wayland limitations    1. Move the Mouse to (x, y) <p>The implementation varies depending on the operating system and windowing system:</p> a. Windows and Linux with X11 <ul> <li> <p>Uses the <code>pyautogui</code> library.</p> </li> <li> <p>Allows absolute positioning and mouse clicks reliably.</p> </li> <li> <p>Implementation is straightforward, no calibration is required.</p> </li> </ul> b. Linux with Wayland <p>Wayland introduces strict sandboxing, meaning that processes can only control their own window. Attempting to move the mouse outside (e.g., to click on another app like Spotify) will fail.</p> Limitations <ul> <li> <p>Wayland blocks <code>pyautogui</code> and similar libraries from performing absolute movements.</p> </li> <li> <p>Tools like ydotool offer relative movement, but they are not stable and don't allow precise targeting.</p> </li> </ul> \u2705 Solution: Calibrated Movement with <code>uinput</code> <p>To overcome these limitations, we use a calibration-based system implemented in:</p> <pre><code>/action_space/experimental/mouse/os_dependant/linux_calibrated_move.py\n</code></pre> How it works: <ol> <li> <p>Calibration File:</p> <ul> <li> <p>A <code>.env</code> file in <code>/action_space/experimental/mouse/</code> contains the ratio between uinput-generated movement and pixels moved on screen.</p> </li> </ul> </li> <li> <p>Reset Position:</p> <ul> <li> <p>Since the current cursor position is unknown, we send a series of <code>uinput</code> events to move the cursor to a known reference (e.g., top-left corner).</p> </li> </ul> </li> <li> <p>Compute Target Movement:</p> <ul> <li> <p>Using the calibration data, we calculate the number of <code>uinput</code> signals needed to reach <code>(x, y)</code> from the reference corner.</p> </li> </ul> </li> <li> <p>Perform Step-by-Step Movement:</p> <ul> <li> <p>Emit the required number of relative movement signals in both <code>x</code> and <code>y</code> directions.</p> </li> </ul> </li> </ol> <p>This method bypasses Wayland restrictions and enables cursor positioning across applications.</p> 2. Perform Click at (x, y) a. Windows <ul> <li> <p>Uses <code>pyautogui.click(x, y)</code> directly.</p> </li> </ul> b. Linux (Wayland) <ul> <li> <p>After reaching <code>(x, y)</code> using the calibrated move:</p> <ul> <li> <p>A mouse click is emulated using the <code>uinput</code> library, by emitting <code>BTN_LEFT</code> events.</p> </li> </ul> </li> </ul> <p>This approach ensures that mouse control is robust and consistent across platforms, even under restrictive environments like Wayland.</p>"},{"location":"packages-apps%E2%80%90management/","title":"Packages apps\u2010management","text":"packages/apps-management <p>The <code>packages/apps-management</code> module provides a collection of actions that enable an agent to search, open, and manage desktop applications on the user's system. These are especially useful for orchestrating workflows across different apps.</p> Actions <code>packages/apps-management.get_opened_windows()</code> <p>Lists all currently opened windows on the machine.</p> <ul> <li> <p>Returns: A list of windows with:</p> <ul> <li> <p><code>name</code>: Title of the window (can reflect content, e.g., a browser tab)</p> </li> <li> <p><code>is_active</code>: Whether the window is currently focused</p> </li> <li> <p><code>path</code>: Path to the executable (if available)</p> </li> </ul> </li> </ul> <p>Example Output:</p> <pre><code>{\"name\": \"Edge - New Tab\", \"is_active\": false, \"path\": \"C:/Program Files/Edge/msedge.exe\"}\n{\"name\": \"Spotify\", \"is_active\": true, \"path\": \"C:/Users/User/AppData/.../Spotify.exe\"}\n</code></pre> <code>packages/apps-management.activate_already_opened_window(name)</code> <p>Brings a specific window to the foreground and maximizes it.</p> <ul> <li> <p>Parameter: <code>name</code> \u2013 Window title as seen in <code>get_opened_windows()</code>.</p> </li> </ul> <p>Example:</p> <pre><code>packages/apps-management.activate_already_opened_window(\"Spotify\")\n</code></pre> <ul> <li> <p>Note: Uses fuzzy matching with a similarity threshold, so minor differences are tolerated.</p> </li> </ul> <code>packages/apps-management.search_for_installed_app(name)</code> <p>Searches for an installed application on the system and returns executable paths.</p> <ul> <li> <p>Parameter: <code>name</code> \u2013 Generic app name (e.g., <code>Edge</code>, <code>Spotify</code>).</p> </li> <li> <p>Returns: List of <code>.exe</code> or <code>.lnk</code> paths.</p> </li> </ul> <p>Example:</p> <pre><code>packages/apps-management.search_for_installed_app(\"Edge\")\n</code></pre> <ul>  &gt; [!WARNING] &gt; Only implemented for Windows. Uses a wrapper to access Windows Search API.  </ul> <code>packages/apps-management.open_app_from_path(path)</code> <p>Opens an application from a known path.</p> <ul> <li> <p>Parameter: <code>path</code> \u2013 Full path to the executable (use <code>/</code> as separator).</p> </li> </ul> <p>Example:</p> <pre><code>packages/apps-management.open_app_from_path(\"C:/Program Files/Edge/Application/msedge.exe\")\n</code></pre> <ul> <li> <p>Note: After execution, it is recommended to call <code>get_opened_windows()</code> to confirm that the app opened.</p> </li> </ul> Suggested Workflow <ol> <li> <p>Check running apps:</p> <pre><code>packages/apps-management.get_opened_windows()\n</code></pre> </li> <li> <p>If the app is not found, locate it:</p> <pre><code>packages/apps-management.search_for_installed_app(\"Edge\")\n</code></pre> </li> <li> <p>Open the application:</p> <pre><code>packages/apps-management.open_app_from_path(\"C:/Program Files/.../Edge.exe\")\n</code></pre> </li> <li> <p>Optionally bring it to focus:</p> <pre><code>packages/apps-management.activate_already_opened_window(\"Edge\")\n</code></pre> </li> </ol> Summary Table  Action | Description -- | -- get_opened_windows() | Lists currently opened windows activate_already_opened_window(name) | Brings a specific window to focus search_for_installed_app(name) | Finds installed apps by name open_app_from_path(path) | Opens an app given its full path   <p>These tools allow agents to automate and manage real-world app workflows in a reliable, platform-aware manner.</p>"},{"location":"packages-sleep/","title":"Packages sleep","text":"packages/sleep <p>The <code>packages/sleep</code> module provides a simple but essential action that allows an agent to pause execution for a specified number of seconds. This is useful when waiting for external events to complete, such as application loading or page rendering. Allowing for some token saving on agentic tasks.</p> Action <code>packages/sleep.sleep(seconds)</code> <ul> <li> <p>Purpose: Pauses the agent's execution for the specified duration.</p> </li> <li> <p>Parameter: <code>seconds</code> (int) \u2014 Number of seconds to sleep.</p> </li> <li> <p>Returns: A confirmation string after sleeping.</p> </li> </ul> <p>Example:</p> <pre><code>packages/sleep.sleep(5)\n</code></pre> <p>This causes the agent to wait for 5 seconds before continuing.</p> Use Cases <ul> <li> <p>Waiting for an application to open after calling <code>open_app_from_path()</code></p> </li> <li> <p>Allowing time for a page to finish loading before performing UI actions</p> </li> <li> <p>Creating delays in scripted automation</p> </li> </ul> Notes <ul> <li> <p>If unsure how long to wait, it is safe to start with 5\u201310 seconds and adjust based on results.</p> </li> <li> <p>You can repeat the action multiple times for longer delays.</p> </li> </ul> Summary  Action | Description -- | -- sleep(seconds) | Pauses execution for a given number of seconds   <p>This action is crucial in asynchronous or slow-loading environments, allowing agents to synchronize more reliably with external systems.</p>"},{"location":"vision-moondream/","title":"Vision moondream","text":"<p>The <code>vision/moondream</code> action allows an agent to ask natural language questions about what is visible on the screen. It captures a screenshot and uses the Moondream Vision-Language model to interpret the image and return an answer. Its a cheaper and faster way to recover information about the screen than using the default LLM.</p> <p>Additionally, this enables agents to understand context or details about the UI without relying on OCR or manual labeling.</p>"},{"location":"vision-moondream/#action","title":"Action","text":""},{"location":"vision-moondream/#visionmoondreamask_to_imagequestion","title":"<code>vision/moondream.ask_to_image(question)</code>","text":"<ul> <li>Description: Takes a screenshot of the screen and sends it along with a question to the Moondream model.</li> <li>Returns: A natural language answer as a string.</li> </ul>"},{"location":"vision-moondream/#parameters","title":"Parameters","text":"<ul> <li><code>question</code> (str): The question to ask about the image.</li> </ul>"},{"location":"vision-moondream/#example","title":"Example","text":"<pre><code>vision/moondream.ask_to_image(\"What app is currently open?\")\n</code></pre> <p>Returns something like:</p> <pre><code>\"Spotify is open and currently playing music.\"\n</code></pre>"},{"location":"vision-moondream/#notes","title":"Notes","text":"<ul> <li>Moondream is optimized for speed and cost, typically responding within \\~1 second.</li> <li>The model can sometimes return inaccurate responses, so critical decisions should be double-checked.</li> </ul>"},{"location":"vision-moondream/#use-cases","title":"Use Cases","text":"<ul> <li>Asking for a description of what's on screen.</li> <li>Reasoning about layout or interface changes.</li> </ul>"},{"location":"vision-moondream/#summary","title":"Summary","text":"<ul> <li>Action: <code>vision/moondream.ask_to_image(question)</code></li> <li>Purpose: Enables agents to reason about visual screen content using natural language.</li> <li>Powered by: Moondream Vision-Language model.</li> <li>Strengths: Fast, cost-effective, text-based image understanding.</li> <li>Limitations: Not ideal for precise point detection or complex spatial reasoning.</li> </ul> <p>Use this action to give agents a quick and general understanding of the visual state of the screen.</p>"}]}